{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274fb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bba742b91375ed640d206a3c9f351e5",
     "grade": false,
     "grade_id": "cell-7dd133571740e15f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 4 - Function Approximators Part 2: Deep Q Network </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Task environments </a>\n",
    "* <a href='#1.2'> 1.2 Learning Objectives </a>\n",
    "* <a href='#1.3'> 1.3 Code Structure & Files </a>\n",
    "* <a href='#1.4'> 1.4 Execution time </a>\n",
    "* <a href='#2.'> 2. A (not-so-)deep Q-network</a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing DQN (10 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Considering Continuous Action Spaces (5 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Continuous Action Spaces Part 1 (15 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> The Target Network (10 points) </a>\n",
    " \n",
    "\n",
    "**Total Points:** 40\n",
    "\n",
    "**Estimated runtime of all the cells:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3bd96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ccf0d452335e813bfc82782639cf8f7",
     "grade": false,
     "grade_id": "cell-ebc6fa2d1ab8fcf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In various real-world scenarios, dealing with high-dimensional state spaces makes it impractical to compute and store Q-values for every possible state-action pair in a Q-table. To address this challenge, we turn to function approximators. In this assignment, you will progress creating a basic Deep Q-Network (DQN) in the **Cartpole** and **LunarLander** environment.\n",
    "\n",
    "**Please start working on this assignment early since the DQN will take some time to train.**\n",
    "\n",
    "\n",
    "<div style=\"display:flex\">\n",
    "     <div style=\"flex:1;padding-left:100px;\">\n",
    "          <img src=\"imgs/cartpole.png\" width=\"300\"/>\n",
    "         <figcaption style=\"flex:1;padding-left:20px;\">  Figure 1: The Cartpole environment. </figcaption>\n",
    "     </div>\n",
    "     <div style=\"flex:1;padding-left:70px;\">\n",
    "          <img src=\"imgs/lunar_lander.png\" width=\"300\"/>\n",
    "         <figcaption style=\"flex:1;padding-left:20px;\">  Figure 2: The Lunarlander environment. </figcaption>\n",
    "     </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "Useful Sources:\n",
    "\n",
    "- Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013). https://arxiv.org/pdf/1312.5602.pdf\n",
    "\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "In this excercise, we will mainly use DQN for two tasks:\n",
    "- Cartpole(https://gymnasium.farama.org/environments/classic_control/cart_pole/): This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "- Lunar Lander (https://gymnasium.farama.org/environments/box2d/lunar_lander/): This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.1'></a>\n",
    "- Understand why and how function approximators can be used for Q-learning\n",
    "- Understand the Deep Q-Network RL algorithm\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "```ex4_dqn.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_rbf.ipynb```** </span>\n",
    "\n",
    "```\n",
    "├───cfg                            # Config files for environments\n",
    "├───imgs                           # Images used in notebook\n",
    "├───results\n",
    "│   └───CartPole-v1\n",
    "│   │   ├───logging                \n",
    "│   │   │    └───logging.pkl        # Contains logged data\n",
    "│   │   ├───model              \n",
    "│   │   │    └───*dqn.pt            # Contains trained model\n",
    "│   │   └───video                   # Videos saved\n",
    "│   │   └───*cartpole_dqn.png       # Contains training performance plot\n",
    "│   └───LunarLander-v2\n",
    "│   │   ├───logging                \n",
    "│   │   │    └───logging.pkl        # Contains logged data\n",
    "│   │   ├───model              \n",
    "│   │   │    └───*dqn.pt            # Contains trained model\n",
    "│   │   └───video                   # Videos saved\n",
    "│   │   └───*lunarlander_dqn.png    # Contains training performance plot\n",
    "│   ex4_dqn.ipynb                   # 2nd assignment file containing tasks <---------This task\n",
    "│   ex4_rbf.ipynb                   # 1st assignment file containing tasks <---------\n",
    "│   train.py                        # Contains train and test functions \n",
    "│   utils.py                        # Contains useful functions \n",
    "└───buffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of these methods might take more than 40 mins depends on the server. If you have problem of experiment running takes too much time, you can download the jupyter notebook and test it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6968238",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47530e48db6901812adae6ba0239bc2d",
     "grade": false,
     "grade_id": "cell-fdd24e05653d4a99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don’t copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba7612-914b-4860-b019-7dc32b075b6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ead3054091bb4a4807d085110e4ebc7",
     "grade": false,
     "grade_id": "cell-3c7108bbf5fe22bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 2. A (not-so-)deep Q-network <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2f2ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a84e8c7719f42c54880e4a2a07f85574",
     "grade": false,
     "grade_id": "cell-af25de8f4a4da8d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing DQN (10 points) </h3> \n",
    "\n",
    "Finish the incomplete code in DQNAgent (functions ```update``` and ```get_action```, marked with ```TODO```) to implement a DQN agent.\n",
    "\n",
    "**See Figure 3 for an example training performance plot for cartpole. Save the training performance plots, and check if they are in the right place (the paths please refer to <a href='#3.'>Submitting<a>).**\n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/dqn.png\" width=\"300px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 3: The training performance plot for cartpole-dqn might look something like this. </figcaption>\n",
    "</figure>\n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a2760",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "578a265f70c27c97dcec1ac8b16d4362",
     "grade": true,
     "grade_id": "cell-6421cfde6887e896",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d14d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch, random, copy, yaml, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import utils as u\n",
    "import train as t\n",
    "\n",
    "from IPython.display import Video # to display videos\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c93182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(in_dim, mlp_dims: List[int], out_dim, act_fn=nn.ReLU, out_act=nn.Identity):\n",
    "    \"\"\"Returns an MLP.\"\"\"\n",
    "    if isinstance(mlp_dims, int): raise ValueError(\"mlp dimensions should be list, but got int.\")\n",
    "\n",
    "    layers = [nn.Linear(in_dim, mlp_dims[0]), act_fn()]\n",
    "    for i in range(len(mlp_dims)-1):\n",
    "        layers += [nn.Linear(mlp_dims[i], mlp_dims[i+1]), act_fn()]\n",
    "    # the output layer\n",
    "    layers += [nn.Linear(mlp_dims[-1], out_dim), out_act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01a546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_shape, n_actions,\n",
    "                 batch_size=32, hidden_dims=[12], gamma=0.98, lr=1e-3, grad_clip_norm=1000, tau=0.001):\n",
    "        self.n_actions = n_actions\n",
    "        self.state_dim = state_shape[0]\n",
    "        self.policy_net = mlp(self.state_dim, hidden_dims, n_actions).to(device)\n",
    "        self.target_net = copy.deepcopy(self.policy_net)\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=float(lr))\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.grad_clip_norm = grad_clip_norm\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.counter = 0\n",
    "\n",
    "    def update(self, buffer):\n",
    "        \"\"\" One gradient step, update the policy net.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        self.counter += 1\n",
    "        # Do one step gradient update\n",
    "        batch = buffer.sample(self.batch_size, device=device)\n",
    "        # Hint:\n",
    "        #    state = batch.state\n",
    "        #    action = batch.action \n",
    "        #    next_state = batch.next_state\n",
    "        #    reward = batch.reward \n",
    "        #    not_done = batch.not_done \n",
    "        \n",
    "        # TODO: Task 3: Finish the DQN implementation.\n",
    "        # Hints: 1. You can use torch.gather() to gather values along an axis specified by dim. \n",
    "        #        2. torch.max returns a namedtuple (values, indices) where values is the maximum \n",
    "        #           value of each row of the input tensor in the given dimension dim.\n",
    "        #           And indices is the index location of each maximum value found (argmax).\n",
    "        #        3.  batch is a namedtuple, which has state, action, next_state, not_done, reward\n",
    "        #           you can access the value be batch.<name>, e.g, batch.state\n",
    "        #        4. check torch.nn.utils.clip_grad_norm_() to know how to clip grad norm\n",
    "        #        5. You can go throught the PyTorch Tutorial given on MyCourses if you are not familiar with it. \n",
    "        # calculate the q(s,a)\n",
    "        ########## You code starts here #########\n",
    "     \n",
    "        \n",
    "        ########## You code ends here #########\n",
    "\n",
    "        # update the target network\n",
    "        u.soft_update_params(self.policy_net, self.target_net, self.tau)\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        update_time = end - start\n",
    "        return {'loss': loss.item(), \n",
    "                'q_mean': qs.mean().item(),\n",
    "                'num_update': self.counter,\n",
    "                'update_time': update_time}\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        # TODO:  Task 3: implement epsilon-greedy action selection\n",
    "        ########## You code starts here #########\n",
    "        p = np.random.uniform(0, 1)\n",
    "        if p < epsilon:\n",
    "            action = ...\n",
    "        else:\n",
    "            action = ...       \n",
    "        ########## You code ends here #########\n",
    "        return action\n",
    "\n",
    "\n",
    "    def save(self, fp):\n",
    "        path = fp/'dqn.pt'\n",
    "        torch.save({\n",
    "            'policy': self.policy_net.state_dict(),\n",
    "            'policy_target': self.target_net.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load(self, fp):\n",
    "        path = fp/'dqn.pt'\n",
    "        d = torch.load(path)\n",
    "        self.policy_net.load_state_dict(d['policy'])\n",
    "        self.target_net.load_state_dict(d['policy_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d97e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'cartpole_dqn.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = DQNAgent(state_shape=cfg.state_shape, n_actions=cfg.n_actions, batch_size=cfg.batch_size, hidden_dims=cfg.hidden_dims,\n",
    "                 gamma=cfg.gamma, lr=cfg.lr, tau=cfg.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc3169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainig takes approximately 40 mins,depends on server\n",
    "if not skip_training:\n",
    "    t.train(agent,  cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml',) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a1164-6330-4e45-ba52-1a84d8a91001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the DQN training plots for the cartpole \n",
    "if not skip_training:\n",
    "    t.plot(save_name='cartpole_dqn.png', cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf80e71",
   "metadata": {},
   "source": [
    "In order to get full points from the task make sure most of the test episodes give a reward of 200, given the seeds below (at least 50% of episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09736f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'cartpole_dqn.yaml', cfg_args=dict(save_video=True), seeds=[67, 23, 89, 12, 45, 78, 34, 90, 21, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3658eb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38a43041b8d9d7219c1c55d322617d12",
     "grade": true,
     "grade_id": "cell-3ef5a6f870ccd026",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63b5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'CartPole-v1'/'video'/'test'/'ex4_dqn-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fa76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'lunarlander_dqn.yaml', 'r') as f:\n",
    "    cfg = t.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = DQNAgent(state_shape=cfg.state_shape, n_actions=cfg.n_actions, batch_size=cfg.batch_size, hidden_dims=cfg.hidden_dims,\n",
    "                 gamma=cfg.gamma, lr=cfg.lr, tau=cfg.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3775e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainig takes approximately 60 mins,depends on server\n",
    "if not skip_training:\n",
    "    t.train(agent, cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a7511-d1aa-4691-be76-406dbccdc739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the DQN training plots for the lunarlander task\n",
    "if not skip_training:\n",
    "    t.plot(save_name='lunarlander_dqn.png', cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74030fd2",
   "metadata": {},
   "source": [
    "In order to get full points from the task make sure most of the test episodes give a reward of 200, given the seeds below (at least 50% of episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2808d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'lunarlander_dqn.yaml', cfg_args=dict(save_video=True),seeds=[67, 23, 89, 12, 45, 78, 34, 90, 21, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85204079",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b145c58866764ca02e0b5c4729621a0",
     "grade": true,
     "grade_id": "cell-7c41d50955488cf2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34889d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "  video = Video(Path().cwd()/'results'/'LunarLander-v2'/'video'/'test'/'ex4_dqn-episode-8.mp4',\n",
    "  embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "  display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55896461",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7672a364f357aec8cb5210d0f941fe63",
     "grade": false,
     "grade_id": "cell-c5606712f2bd514a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Considering Continuous Action Spaces (5 points) </h3> \n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95154b-026f-4e95-9492-20782e8aa24d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f6c4dc6645b2c94d0cbd2e55e2ebe45",
     "grade": false,
     "grade_id": "cell-d77b9f2562ab2797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.1.1 Question (5 points):\n",
    "Can Q-learning be used directly in environments with continuous action spaces?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. Yes, Q-learning can be used directly without any modifications.\n",
    "2. No, Q-learning cannot be used directly because it only works with a discrete set of actions.\n",
    "3. No, Q-learning cannot be used directly in continuous action spaces, but it can be adapted with certain modifications.\n",
    "4. Yes, Q-learning can be used directly, but it is less efficient than in discrete action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22025c94-158f-4ee7-834a-1248eb189d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq_1_1_1 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2290f-67a7-4fe8-9a55-7db7bb6c88f6",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf5784-303d-4aa9-b036-75219830d49d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85bc371a65d530f9fc963804e483295d",
     "grade": true,
     "grade_id": "cell-9b579c6ba03d991a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq_1_1_1 in range(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42005b40-9ed9-4977-bcbf-8420c8d177b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cca2254b439447f716776ca19336a27",
     "grade": true,
     "grade_id": "cell-254ea90497e5d82c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1395f3d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "294b5606dc64ab643d5b5cb9823f7068",
     "grade": false,
     "grade_id": "cell-ca3df96c77733d4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Continuous Action Spaces Part 1 (15 points) </h3> \n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee6c44-ca32-4620-85c5-204a4ef2646d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "368c9ec46e655d5206fd92a3afd6452c",
     "grade": false,
     "grade_id": "cell-19c97802050ed143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2.1 Question (5 points):\n",
    "\n",
    "What is the primary challenge when applying DQN to a continuous action space? Hint: select the one that describes the main issue.\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. Updating the Q values\n",
    "2. Storing the Q values\n",
    "3. Performing the maximization (argmax) operation\n",
    "4. Calculating the loss function\n",
    "5. Calculating the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f0ef2-5e19-41d8-bf49-3c8f38ffe645",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_2_1 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c67c7f-cabd-40df-97a6-8ddbcfa9a8d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed192a4eb4250673f4af98c6e743ef23",
     "grade": false,
     "grade_id": "cell-97197487f8ceeb64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2.2 Question (5 points):\n",
    "\n",
    "Which one of the following methods does NOT address the continuous action space problem in DQN?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. Learn a function approximation for the maximum of the Q-function\n",
    "2. Use a policy to draw samples of Q(s,a) and select the action with the highest Q-value (e.g., MPO)\n",
    "3. Approximate the Q-function as a convex function\n",
    "4. Use a Deep Deterministic Policy Gradient (DDPG) approach to learn a policy and Q-function simultaneously\n",
    "5. Use a Double DQN approach with two separate networks to estimate Q-values for continuous actions\n",
    "6. Discretize the action space with fine-grained intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ae7de-289e-44ab-bfed-437273e38cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_2_2 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf50e1-f1d0-4735-a614-adffd7574619",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccae140251592e9baae2e607b9db5a96",
     "grade": false,
     "grade_id": "cell-f42263c476c89d1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.2.3 Question (5 points):\n",
    "\n",
    "Discretization is one approach to address the continuous action space problem. What is a **significant** limitation of discretizing the action space to address the continuous action space problem in DQN?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. It may not generalize well to actions outside the discrete set\n",
    "2. It increases the size of the state space exponentially\n",
    "3. It requires modifying the neural network architecture\n",
    "4. It can lead to unstable learning in the early stages of training\n",
    "5. It makes the reward function non-differentiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ba924-6705-4982-8477-93f27336b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_2_3 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695dc82-6d20-4051-9e8e-1e256a5bf8fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea9c2ff3ff7fa62ff869bc2e92a32207",
     "grade": false,
     "grade_id": "cell-6d65dca0030ecedc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf9407-26e1-4b81-910f-a877ebd3d68e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e70d5f938adbf449599993ed8984ee7",
     "grade": true,
     "grade_id": "cell-b7c78664ec74ec2a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq_1_2_1 in range(1, 6)\n",
    "assert sq_1_2_2 in range(1, 7)\n",
    "assert sq_1_2_3 in range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c89084-b34b-4b3b-bb23-6d84d3b17180",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "261ad454b4494ded2716bb3ea7da00f4",
     "grade": true,
     "grade_id": "cell-cee299bf0c9ac8e4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4eeeb7-aa75-49f5-b69a-3a157060ef58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0166d8f9297188b4711c3a925bf6006",
     "grade": true,
     "grade_id": "cell-bd84024a26ba389c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11daf090-e4f1-4ca7-bd2a-9e786c3d794b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14e5eefe2c878d41a5c63ece44a2ab4",
     "grade": true,
     "grade_id": "cell-42e6f022fdf27684",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10e680f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84e1103ca1d0b41cb9afb1cceed72204",
     "grade": false,
     "grade_id": "cell-640e21ebd354971a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> The Target Network (10 points) </h3> \n",
    "    \n",
    "🔝\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f15fdf-5b08-469f-a0d5-f63f6a969aff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61cefb26ece3864427822aed99841924",
     "grade": false,
     "grade_id": "cell-a748a52ab72e0273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.3.1 Question (5 points):\n",
    "\n",
    "Why do we use a separate target network in DQN instead of using the same network for both Q(s,a) and max_a(Q(s', ·))?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. To prevent the network from overfitting to the most recent transitions\n",
    "2. To ensure that the Q-values are always within a specific range\n",
    "3. To stabilize the training process by preventing frequent changes in target values\n",
    "4. To enable the network to learn from both on-policy and off-policy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559704b6-23fd-4973-92ba-6492923011e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_3_1 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24ade0-0094-401f-820d-956a237e2247",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e10a194b9212ed25ee1f1a223182e825",
     "grade": false,
     "grade_id": "cell-e43b228069f05456",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### 1.3.2 Question (5 points):\n",
    "\n",
    "What would happen if we don't stop the gradient of the target Q-value in DQN?\n",
    "\n",
    "**Select one:**\n",
    "\n",
    "1. The optimization process would become more stable due to the joint updates\n",
    "2. The target network updates would be synchronized with the main network updates\n",
    "3. Both the current Q-values and the target Q-values would be updated during optimization\n",
    "4. The algorithm would effectively switch to an on-policy learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7714624-1df2-4e67-aa6a-0b248f69981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_1_3_2 = 0 # Replace with the appropriate answer number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dd848-e39f-4fa8-bd6c-4b4c38396412",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87ced07276419b634e8b7d852523bc90",
     "grade": false,
     "grade_id": "cell-2f5dabf7c2e0b401",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b45bf0-c052-4b1c-b19c-750b371a6cf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738fa24b9d702b770e751fa6df17c2a3",
     "grade": true,
     "grade_id": "cell-7427871eee3676e5",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sq_1_3_1 in range(1, 5)\n",
    "assert sq_1_3_2 in range(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be39cf-a443-4ce7-b709-4701b5b9d187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f7a58a67e676406e37624e5050e1132",
     "grade": true,
     "grade_id": "cell-06da3985fcfd8e51",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fd766-1305-43dd-8a4f-756ad0457882",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a767e7d006960d5cd4bf0181c632ea3",
     "grade": true,
     "grade_id": "cell-a8c5938cdf0dfd2c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4266d794-008d-4d2c-9a9e-0e18d9b6e354",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex4_dpn.ipynb``` and ```ex4_rbf.ipynb```) are answered and and that the necessary plots are saved in the appropriate locations. The relevant plots and files needed to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `cartpole_dqn.png`: Cartpole, training performance plots in terms of episode and episodic reward\n",
    "  - `lunarlander_dqn.png`: Lunarlander, training performance plots in terms of episode and episodic reward \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "  \n",
    "\n",
    "- Model files:\n",
    "  - `dqn.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/CartPole-v1/cartpole_dqn``` Training result for Cartpole environment\n",
    "- ```results/CartPole-v1/model/dqn.pt``` Model for Cartpole environment\n",
    "- ```results/LunarLander-v2/lunarlander_dqn.png``` Training result for LunarLander environment\n",
    "- ```results/LunarLander-v2/model/dqn.pt``` Model for LunarLander environment\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex4_rbf.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8536b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc73e5f68adbc298f70feabf9026032d",
     "grade": true,
     "grade_id": "cell-c97f1e23824070fc",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec40b02-69c6-401a-ad79-2f67a8a660e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8baf6-b860-4338-b655-b28ec4fac282",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94026d-ad80-4148-8e02-5bcfd4b33e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e1e56-4c4f-4bdf-95b3-f4efcc4c053a",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f647d-4cf5-4cf8-abbf-1c0d285c5f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DQN (10 points)\n",
    "Q1_1 = None # Question 1.1 Considering Continuous Action Spaces (5 points)\n",
    "Q1_2 = None # Question 1.2 Continuous Action Spaces Part 1 (15 points)\n",
    "Q1_3 = None # Question 1.3 The Target Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4a6d0-4ff1-4705-93af-493eed829e4e",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb003431-4a23-4350-86ec-34444a64ee9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DQN (10 points)\n",
    "Q1_1 = None # Question 1.1 Considering Continuous Action Spaces (5 points)\n",
    "Q1_2 = None # Question 1.2 Continuous Action Spaces Part 1 (15 points)\n",
    "Q1_3 = None # Question 1.3 The Target Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa9a62-dcf9-41fe-8d56-a852b2582efa",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edf97b-6cd9-48a9-bbc9-a075e6c15abb",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
