{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba46938f",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Project - Sanding Task </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching materials for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2024 - Nov 30, 2024</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Learning Objectives</a>\n",
    "* <a href='#2.'> 2. Introduction </a>\n",
    "* <a href='#3.'> 3. Sanding Task </a>\n",
    "* <a href='#4.'> 4. Code Structure and Files </a>\n",
    "* <a href='#5.'> 5. Tasks </a>\n",
    "* <a href='#6.'> 6. Implementation requirements </a>\n",
    "* <a href='#7.'> 7. Evaluation / grading </a>\n",
    "* <a href='#8.'> 8. Start the project </a>\n",
    "* <a href='#9'> 9. Submission </a>\n",
    "* <a href='#10'> 10. Feedback  </a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implement the basic PPO or DDPG algorithm. Modify either 'ddpg_agent.py' or 'ppo_agent.py' in the folder 'algos'. Run the algorithm in the easy and medium sanding environments. Report the results, that is, training plots and test performances. (30 points)</a>\n",
    "\n",
    "<a href='#T2'><b>Student Task 2.</b> Extend DDPG/PPO to improve the performance based on the hints in Section 5. Note the instructions on the extensions in Section 5. You can should use either the file 'ddpg_extension.py' or 'ppo_extension.py' in folder 'algos'. Do the following: 1. The base algorithm's performance must be improved such that the agent succeeds in the moderate difficulty environment. 2. Follow the provided structure such that it can be tested with function 'test(agent)'. (40 points)</a>\n",
    "\n",
    "<a href='#Q1'><b>Student Question 1.</b> Answer the question regarding how you extended PPO/DDPG. (30 points) </a>\n",
    "\n",
    "<a href='#T3'><b>Student Task 3.</b> This task gives bonus points to the project works that get highest performance. The projects will be evaluated based on their performance in the difficult Task 3 environment and then ranked. The best performing project (100% ranked) will receive 20 bonus points, 95% ranked or above will get 10 bonus points. (+20 points) </a>\n",
    "\n",
    "**Total Points:** 100 (+20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2e19d-6a45-4472-94a7-cbe7f66c11fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color:lightcoral; padding:10px; border-radius:5px\">\n",
    "\n",
    "# <span style=\"color:white\">0. Group Information (IMPORTANT)</span>\n",
    "\n",
    "## <span style=\"color:white\">Please read the instructions and fill in your group information</span>\n",
    "\n",
    "- This project work is intended to be completed in groups of 2 students, who will share the same grade. If you are looking for a project partner, please join the project channel on Zulip and introduce yourself. \n",
    "\n",
    "- However, it is also acceptable to complete the project individually. \n",
    "\n",
    "- **Only one student** from each group should submit the project.\n",
    "\n",
    "- Please provide the **NAME (First Name Last Name)**, **Aalto Student ID**, and **Aalto User Name** of each group member. For example:\n",
    "    - Member 0: Jane Doe, 123456, janed5\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149679af-1b65-44b7-8e8a-5bda39706371",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color:lightcoral; padding:10px; border-radius:5px; margin-bottom: 10px\">\n",
    "\n",
    "# <span style=\"color:white\">### TODO: Fill in Your Group Information HERE</span>\n",
    "\n",
    "DOUBLE CLICK TO EDIT\n",
    "- **Member 1:**\n",
    "  - Name:\n",
    "  - Aalto Student ID:\n",
    "  - Aalto User Name:\n",
    "\n",
    "- **Member 2 (if applicable):**\n",
    "  - Name:\n",
    "  - Aalto Student ID:\n",
    "  - Aalto User Name:\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de82751-fa7f-43fd-be23-890898fb184c",
   "metadata": {},
   "source": [
    "# 1. Learning Objectives <a id='1.'></a>\n",
    "In the project work, students move to a more independent working style compared with the exercises. In the exercises, instructions and template code for reinforcement learning algorithms were provided. However, in the project work the students are given **a new task** that they need to solve using reinforcement learning methods discussed during the course. The students need to decide which method they will use (either PPO or DDPG), extend the method, and explain why. Students may take advantage of code that they have already developed in the exercises or which was part of the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4535f3",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Introduction <a id='2.'></a>\n",
    "\n",
    "The goal of the project work is to optimize the behavior of a sanding robot using reinforcement learning such that the robot avoids already painted areas but sands areas that need sanding. We give now a general motivation and task description. [Section 3](#3.) provides a more detailed task definition.\n",
    "\n",
    "<center>\n",
    "<figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "  <img src=\"imgs/robot_sanding.png\" width=\"width:40%\"/>\n",
    "</figure>\n",
    "</center>\n",
    "<center>Figure 1: Illustration of a robot sanding a planar area. Image source: (https://www.youtube.com/watch?v=TG-3NZzoZiM)</center>\n",
    "\n",
    "The robot operates on a 2-D plane and should hit a set of \"sanding areas\" using the sanding tool equipped. At the same time the robot has to avoid specific \"painted areas\" which are also defined as part of the system state. The sanding areas and painted areas are defined as part of the state space. The robot gets a negative reward for hitting painted areas and a positive reward for sanding sanding areas.\n",
    "\n",
    "The idea is to optimize the behavior of the robot using reinforcement learning based on either the PPO or DDPG algorithm. You should extend PPO or DDPG such that you get higher performance in the more challenging versions of the sanding task.\n",
    "\n",
    "[Section 3](#3.) defines the sanding task. [Section 4](#4.) shows the structure of the provided file directory. [Section 5](#5.) discusses the **mandatory project requirements** and **possible extensions** to the basic PPO and DDPG algorithms. [Section 6](#6.) discusses the implementation requirements. [Section 7](#7.a) describes how the project is evaluated and graded. In [Section 8](#8.), you will add your implementation, perform the tasks and answer questions. [Section 9](#9.) is a preliminary check. In [Section 10](#10.), you can provide feedback on the project work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4576153-9bd5-4021-ac64-02a9d5f5169f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Sanding Task <a id='3.'></a>\n",
    "The primary objective in this project is to optimize a sanding robot's behavior, aiming to maximize the expected cumulative reward $ J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right] $. The simulator for the sanding task is provided in the \"sanding.py\" Python file. This section defines the sanding task. Please, see below.\n",
    "\n",
    "## 3.1. Markov Decision Process (MDP)\n",
    "- **Robot Characteristics**: The robot is visualized as a <span style=\"color:purple\">purple</span> circle with a radius of 10, operating on a 2D plane. The x and y coordinates range from -50 to 50.\n",
    "- **Sanding & No-Sanding Areas**: There are sanding (<span style=\"color:green\">green</span>) and no-sanding (<span style=\"color:red\">red</span>) areas, each with a radius of 10. Their configurations vary based on the task.\n",
    "  \n",
    "### 3.1.1. State Representation\n",
    "A state \\( s \\) is defined as:\n",
    "\n",
    "$s = [(x_{\\text{ROBOT}}, y_{\\text{ROBOT}}), (x_{\\text{SAND}}, y_{\\text{SAND}})_1, \\dots,\n",
    "    (x_{\\text{SAND}}, y_{\\text{SAND}})_N, (x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_1, \\dots,\n",
    "    (x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_M)]$\n",
    "\n",
    "- $N$ is the number of sanding areas (circles)\n",
    "- $M$ is the number of no-sanding areas (circles) \n",
    "- $(x_{\\text{ROBOT}}, y_{\\text{ROBOT}})$ : Robot's current location\n",
    "- $(x_{\\text{SAND}}, y_{\\text{SAND}})_i$: Location of the $i$th sanding area\n",
    "- $(x_{\\text{NOSAND}}, y_{\\text{NOSAND}})_j$: Location of the $j$th no-sanding area\n",
    "\n",
    "### 3.1.2. Action Space\n",
    "\n",
    "An action $a$ consists of target coordinates for the robot:\n",
    "\n",
    "$a = (a_x, a_y) \\in \\mathbb{R}^2$\n",
    "\n",
    "$a_x, a_y$ selects the current target coordinates of the robot arm. A PD-controller~\\cite{X} trys to move the robot arm from the current coordinates $(x_{\\text{ROBOT}}, y_{\\text{ROBOT}})$ to the target coordinates $a_x, a_y$. You do not need to necessarily understand how exactly the PD-controller works but the controller may not always move the robot to the correct coordinates in one time step, and, it may also overshoot the target location. Please, see below for a visualization of this behavior.\n",
    "\n",
    "\n",
    "### 3.1.3. Reward definition\n",
    "\n",
    "The reward is defined as the number of sanding locations the robot touches minus the number of no-sanding locations the robot touches, that is,\n",
    "$r_t = $ number of sanded sanding locations - number of sanded no-sanding locations . \n",
    "\n",
    "The robot can only sand a sanding or no-sanding location once. All sanding and no-sanding locations that are touched by the robot will be moved outside the operating area, that is, those locations will be outside the operating area in the subsequent time step. \n",
    "\n",
    "\n",
    "## 3.2 Scenarios with different difficulty levels: <a id='3.1'></a>\n",
    "\n",
    "\n",
    "### Environment Breakdown\n",
    "#### Environment 1: Easy Environment \n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display:inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/easy_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "- **No-Sanding Spots**: 1 (<span style=\"color:red\">red</span>), fixed location in the center\n",
    "- **Sanding Spots**: 1 (<span style=\"color:green\">green</span>), randomly generated without overlapping\n",
    "- **PD Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n",
    "\n",
    "#### Environment 2: Moderate Difficulty Environment \n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/middle_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "- **No-Sanding Spots**: 2 (<span style=\"color:red\">red</span>), fixed locations along the diagonal\n",
    "- **Sanding Spots**: 2 (<span style=\"color:green\">green</span>), randomly generated without overlapping\n",
    "- **Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n",
    "\n",
    "#### Environment 3: Difficult Environment \n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/difficult_env.gif\" alt=\"Easy Environment\" width=\"180\"/>\n",
    "  </figure>\n",
    "</p>\n",
    "\n",
    "- **No-Sanding Spots**: 4 (<span style=\"color:red\">red</span>), fixed locations as a square\n",
    "- **Sanding Spots**: 4 (<span style=\"color:green\">green</span>), one of which is fixed in the center while other three are ramdomly generated without overlapping\n",
    "- **Control**: Movement to target \\((a_x, a_y)\\) with random number of PD iterations\n",
    "- **Notes**: Hollow circle indicates target, <span style=\"color:purple\">purple</span> circle is the robot\n",
    "\n",
    "# 4 Code Structure & Files <a id='4.'></a>\n",
    "\n",
    "```project.ipynb``` is the main file needed to be modified for this project, but you can also add other auxiliary files if needed.  \n",
    "```\n",
    "├───cfg\n",
    "│   ├───algo                       # Algorithm configurations\n",
    "│   ├───envs                       # Environment configurations\n",
    "├───imgs                           # Images used in the notebook task description   \n",
    "├───utils                          # Utility functions  \n",
    "├───algo                           # Your agents (PPO/DDPG)\n",
    "├───results                        # Training results                          \n",
    "├───project.ipynb                  \n",
    "└───sanding.py                     # Sanding environment\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## 4.1 Execution time <a id='4.1'></a>\n",
    "\n",
    "The training of DDPG/PPO may take more than 30 min for each run depending on the server load. If you have problems with the training time, you can train DDPG/PPO locally on your computer or a server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b023f1910632940",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Tasks <a id='5.'></a>\n",
    "\n",
    "## I. Tasks 1: use following algorithms to solve the robot sanding task. \n",
    "  - **PPO** (you can take code from ex1 as a basis)\n",
    "  - **DDPG** (you can take code from ex6 as a basis)\n",
    "\n",
    "Implementations from the exercises can be used and extended, otherwise please implement algorithms yourself. For learning purposes, you can look at existing implementations on the Internet.\n",
    "\n",
    "- We do not want to focus on hyperparameter or neural network architecture tuning. Therefore, use the following choices:\n",
    "  - For PPO, use the neural network policy configuration found in exercise 1 code.\n",
    "  - For DDPG, use the neural network policy and value function configuration found in exercise 6 code.\n",
    "  - <span style=\"color:red\">We provide the hyper-parameters in configuration files 'cfg/algo/*', DO NOT change the parameters there</span>\n",
    "\n",
    "- **No copying of code! Code should be original, written by yourself or taken from the exercises.**\n",
    "\n",
    "- You should extend or modify your basic PPO/DDPG algorithm. For each modification or extension, answer the multiple choice questions.\n",
    "\n",
    "- More detailed instructions are given in the following sections. If you are not sure what you are allowed or not allowed to do, contact the TAs, preferably on Zulip so that also others may learn from the question.\n",
    "\n",
    "**Note: not following the requirements may lead to point deduction or rejection of the project work.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## II. Task 2: possible extensions to improve performance\n",
    "\n",
    "After the basic PPO/DDPG implementation, you shall try to do some technical improvements to improve the agent's performance. Below we list several possible extensions you can apply to improve the performance. \n",
    "- <span style=\"color:red\">If you choose to implement PPO, for task 2 you can choose either Dual Clip PPO or Self imitation learning</span>\n",
    "- <span style=\"color:red\">If you choose to implement DDPG, for task 2 you can choose either LNSS or Self imitation learning</span>\n",
    "- <span style=\"color:red\">You should read the paper to do your own implementation and answer the related questions in Student Questions 1.</span>\n",
    "- <span style=\"color:red\">The codes shall be implemented in \"algos/ddpg_extension.py\" or \"algos/ppo_extension.py\"</span>\n",
    "\n",
    "### PPO based Extension: **Dual Clip PPO** \n",
    "- Implement it if your base algorithm is PPO.\n",
    "- It is designed to constrain updates to the policy, effectively preventing it from diverging excessively from its preceding iterations. This approach thereby ensures a more stable and reliable learning process during training. \n",
    "- Ref: ([Ye et al., 2020](https://ojs.aaai.org/index.php/AAAI/article/view/6144)).\n",
    "\n",
    "\n",
    "\n",
    "### DDPG based Extension: **LNSS**\n",
    "- Implement it if your base algorithm is DDPG.\n",
    "- Its core idea is to improve N-step bootstrapping for off-policy RL like DDPG. It was empirically suggested that n-step methods perform better than single-step methods since n-step returns propagate rewards to relevant state-action pairs more efficientlyy.\n",
    "- Ref([Junmin et al., 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/29ef811e72b2b97cf18dd5d866b0f472-Paper-Conference.pdf)).\n",
    "\n",
    "### General Extension: **Self imitation learning**\n",
    "- <span style=\"color:red\">Either you implemented PPO or DDPG, you can choose to implement it instead of Dual Clip PPO or LN</span>\n",
    "- If you want to try a more general trick that can be used for any RL, you can implement self-imitatin learning.\n",
    "- It exploits past good experiences to drive deep exploration. \n",
    "- Ref:([Oh et al., 2018](https://proceedings.mlr.press/v80/oh18b.html)).\n",
    "\n",
    "## III. Hints & Tips\n",
    "\n",
    "### a) Hints\n",
    "- Due to the multidimensional actions, when using a Gaussian policy, remember to use a multivariate Gaussian probability distribution, or, product of standard Gaussian distributions that corresponds to a multivariate Gaussian distribution with a diagonal covariance matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce5c40",
   "metadata": {},
   "source": [
    "# <span> 6. Implementation requirements </span> <a id='6.'></a>\n",
    "\n",
    "### <span> Your implemented algorithm must be compatible with the below requirements. This is the default behavior, however make sure not to disrupt it.</span>\n",
    "\n",
    "## I. Data Saving Format\n",
    "\n",
    "### a) Training Logs: \n",
    "During the training, your code shall create a CSV file as the training log.\n",
    "\n",
    "  - This log should output a CSV file with the following format:\n",
    "    ```\n",
    "    ,episode_length,ep_reward,episodes,total_step,average_return\n",
    "    0,20,0.0,99,2000,-0.13\n",
    "    1,20,0.0,199,4000,0.02\n",
    "    ...\n",
    "    ```\n",
    "  - The training log should be saved as:\n",
    "    `results/<environment name>/<algorithm name>/logging/logs_<seed number>.csv`\n",
    "    \n",
    "    For example:\n",
    "    `results/SandingEnvDifficult/ddpg/logging/logs_0.csv`\n",
    "    \n",
    "\n",
    "### b) Model Weights\n",
    "\n",
    "During/after training, the policy/critic weights should be saved in the path:\n",
    "\n",
    "  `results/<environment name>/<algorithm name>/model/model_parameters_<seed number>.pt`\n",
    "  \n",
    "  For example:\n",
    "  \n",
    "  `results/SandingEnvDifficult/ddpg/model/model_parameters_0.pt`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## II. Visualization Plot Functions\n",
    "\n",
    "Ensure that your implemented algorithm is compatible with the functions located in `utils/common_utils.py`:\n",
    "\n",
    "- **Single Training Curve**: \n",
    "  - Function: `plot_reward(path, seed, env_name)`\n",
    "  - Description: Plots the training curve of a single algorithm, trained with a specific seed.\n",
    "  \n",
    "- **Multiple Training Curves**: \n",
    "  - Function: `plot_algorithm_training(path, seeds, env_name)`\n",
    "  - Description: Plots the training curves of a single algorithm, trained with multiple specific random seeds.\n",
    "  - Example: `seeds=[0,1,2]`\n",
    "  \n",
    "- **Comparison of Training Performances**: \n",
    "  - Function: `compare_algorithm_training(algo1, algo2, seeds)`\n",
    "  - Description: Given two configured algorithms/agents, this function will generate comparison plots of their training performances.\n",
    "\n",
    "\n",
    "## III. <span style=\"color:red\"> Configurations </span>\n",
    "\n",
    "We provide the hyperparameters in 'cfg/algo', use those configuration files to initialize your agent. Example refer to following code. You should not change any parameters there.\n",
    "\n",
    "**Usage Example**:\n",
    "```python\n",
    "config = setup(algo='ppo', env='middle')\n",
    "    config[\"seed\"] = 0\n",
    "    agent = PPOAgent(config)\n",
    "```\n",
    "\n",
    "## IV. <span style=\"color:red\"> Training implementation </span>\n",
    "\n",
    "To ensure compatibility with the setup function, your implemented algorithm must follow the specified protocol:\n",
    "\n",
    "- **Function**: `setup(algo=None, env='easy', cfg_args={})`\n",
    "  - **Purpose**: Used for setting up the configurations.\n",
    "  - **Usage Example**:\n",
    "    ```python\n",
    "    config = setup(algo='ppo', env='middle')\n",
    "    config[\"seed\"] = 0\n",
    "    agent = PPOAgent(config)\n",
    "    agent.train()\n",
    "    ```\n",
    "  - **Parameters**:\n",
    "    - `algo`: Specify the algorithm. Use either `'ppo'` or `'ddpg'`.\n",
    "    - `env`: Specify the environment. Options include `'easy'`, `'middle'`, or `'difficult'`.\n",
    "\n",
    "## V. <span style=\"color:red\"> Test implementation </span>\n",
    "\n",
    "To ensure compatibility with the setup function, your implemented algorithm must follow the specified protocol:\n",
    "\n",
    "- **Function**: `test`\n",
    "  - **Purpose**: Used for test the performance of the agent.\n",
    "  - **Usage Example**:\n",
    "    ```python\n",
    "    test(agent, env_name='easy', algo_name='ddpg')\n",
    "    ```\n",
    "\n",
    "## VI. <span style=\"color:red\"> Access </span>\n",
    "\n",
    "You should not modify any files in paths:\n",
    "- 'cfg/'\n",
    "- 'utils/'\n",
    "- 'sanding.py'\n",
    "\n",
    "## VI. <span style=\"color:red\"> Other tips </span>\n",
    "\n",
    "1. If you choose DDPG based on ex6, you should be careful with function 'get_action()'. You should not use \n",
    "```python\n",
    "    if self.buffer_ptr < self.random_transition:\n",
    "``` \n",
    "if you want to evluate the agent's performance when using:\n",
    "```python\n",
    "test(agent, env_name='easy', algo_name='ddpg')\n",
    "```\n",
    "\n",
    "2. The standard deviation of gaussian policy in PPO is crucial for performance.\n",
    "\n",
    "### <span style=\"color:red\">Your implemented algorithm must be compatible with the above requirements; otherwise, the code will be considered invalid.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb37c5-a697-4d88-9f97-48d123db2983",
   "metadata": {},
   "source": [
    "# 7. Evaluation / Grading <a id='7.'></a>\n",
    "\n",
    "### Grading\n",
    "The general evaluation and grading process will take into account the following aspects:\n",
    "\n",
    "- In Tasks 1, the grading will be based on the performance comparison between your algorithms and the respective baseline. \n",
    "    - If your implementation is able to run successfully, you get 10 points.\n",
    "    - <span style=\"color:red\"> In order to get the full points of task 1, your implementation must have no statistically significant difference than our baseline performance (the average performance is provided below) </span>.\n",
    "    - We implmented the t-test in a hidden cell to automatically do the significant test and grading.\n",
    "- For Task 2 the grading will focus on the implemented extensions. \n",
    "    - If your extension is able to run successfully, you get 10 points.\n",
    "    - <span style=\"color:red\"> In order to get the full points of task 2, the extension must have statistically significant better performance than the base algorithm </span>. \n",
    "    - We implmented the t-test in a hidden cell to do the check.\n",
    "- In Question 1, the evaluation will consider the created extensions. You will get points by correctly answering the given questions.\n",
    "- In Task 3, there will be a competitive grading approach: all projects will be evaluated based on their performance in the difficult Task 3 environment and then ranked. The best performing project (100% ranked) will receive 20 bonus points, 95% ranked or above will get 10 bonus points.\n",
    "\n",
    "### Baseline Performance:\n",
    "\n",
    "The following values were recorded after running experiments for the base algorithms in different environments. Differences due to random conditions are expected.\n",
    "\n",
    "#### For PPO:\n",
    "- PPO (Easy Environment): mean: 0.75\n",
    "- PPO (Middle Environment): mean: 1.14\n",
    "\n",
    "#### For DDPG:\n",
    "- DDPG (Easy Environment): mean: 0.75\n",
    "- DDPG (Middle Environment): mean: 1.19\n",
    "\n",
    "\n",
    "#### Note:\n",
    "    - The PPO baseline is derived from the code used in Exercise 1, featuring a Gaussian policy with an isotropic covariance matrix. (For more details, please google \"isotropic covariance matrix\"). The log standard deviation of the policy is linearly scheduled from the initial log standard deviation to zero during the training.\n",
    "\n",
    "    - The DDPG baseline originates from the code used in Exercise 6. The exploration noise added to the action follows a Gaussian distribution (mean is 0 and standard deviation is 0.3).\n",
    "\n",
    "# 8. Start the project <a id='8.'></a>\n",
    "\n",
    "This section contains below all the source code including your implementation and the tasks and question that need to be filled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa046e7",
   "metadata": {},
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Initialize the code</b> </h3>\n",
    "    Run the following section to start the task. DO NOT MODIFY THE CODE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af4937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make sure to change this part before submission\n",
    "skip_training = False  # Set this flag to True before validation and submission\n",
    "\n",
    "# Make sure to change the algorithm_implemented variable to the algorithm you implemented\n",
    "# This variable will be used in the evaluation and will determine which environment to use\n",
    "# for both the baseline and the extension\n",
    "\n",
    "algorithm_implemented = ... # \"ddpg\" or \"ppo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a14cec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d9963b7120f3eb4c14e32d59a2e411a",
     "grade": true,
     "grade_id": "cell-4109593718f2c17d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dec74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install  imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61638d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np \n",
    "from types import SimpleNamespace as SN\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import utils.common_utils as cu\n",
    "from utils.recorder import RecordVideo\n",
    "\n",
    "# uncomment the following line to import the agent\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "# from algos.ppo_agent import PPOAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70bace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T14:36:27.520118741Z",
     "start_time": "2023-10-29T14:36:27.505961926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to test a trained policy\n",
    "def test(agent, env_name, algo_name):\n",
    "    # Load model\n",
    "    agent.load_model()\n",
    "    print(\"Testing...\")\n",
    "    total_test_reward, total_test_len = 0, 0\n",
    "    returns = []\n",
    "    \n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo_name}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env_name}_env.yaml', 'r'))\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/env_cfg[\"env_name\"]/algo_name\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    \n",
    "    for ep in range(agent.cfg.test_episodes):\n",
    "        seed = rng.integers(low=1, high=1000)\n",
    "        observation, _ = agent.env.reset(seed=int(seed))\n",
    "        test_reward, test_len, done = 0, 0, False\n",
    "        \n",
    "        if ep%100==0:\n",
    "            frames = []\n",
    "\n",
    "            while not done and test_len < agent.cfg.max_episode_steps:\n",
    "                action, _ = agent.get_action(observation, evaluation=True)\n",
    "                observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "                fs = agent.env.render()\n",
    "                frames = frames+fs\n",
    "                test_reward += reward\n",
    "                test_len += 1\n",
    "            total_test_reward += test_reward\n",
    "            total_test_len += test_len\n",
    "            returns.append(test_reward)\n",
    "            cu.save_rgb_arrays_to_gif(frames, video_test_dir/('_seed_'+str(agent.seed)+'_ep_'+str(ep)+'.gif'))\n",
    "        else:\n",
    "            \n",
    "            while not done and test_len < agent.cfg.max_episode_steps:\n",
    "                action, _ = agent.get_action(observation, evaluation=True)\n",
    "                observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "                test_reward += reward\n",
    "                test_len += 1\n",
    "            total_test_reward += test_reward\n",
    "            total_test_len += test_len\n",
    "            returns.append(test_reward)\n",
    "       \n",
    "\n",
    "    print(f\"Average test reward over {len(returns)} episodes: {total_test_reward/agent.cfg.test_episodes},+- {np.std(np.array(returns))}; \\\n",
    "        Average episode length: {total_test_len/agent.cfg.test_episodes}\")\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62e980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup: read the configurations and generate the environment.\n",
    "def setup(algo=None, env='easy', cfg_args={}, render=False, train_episodes=None):\n",
    "    # set the paths\n",
    "    cur_dir=Path().cwd()\n",
    "    cfg_path= cur_dir/'cfg'\n",
    "    \n",
    "    # read configuration parameters:\n",
    "    cfg={'cfg_path': cfg_path, 'algo_name': algo}\n",
    "    env_cfg=yaml.safe_load(open(cfg_path /'envs'/f'{env}_env.yaml', 'r'))\n",
    "    algo_cfg=yaml.safe_load(open(cfg_path /'algo'/f'{algo}.yaml', 'r'))\n",
    "    cfg.update(env_cfg)\n",
    "    cfg.update(algo_cfg)\n",
    "    cfg.update(cfg_args)\n",
    "    \n",
    "    # forcely change train_episodes\n",
    "    if train_episodes is None:\n",
    "        True\n",
    "    else:\n",
    "        cfg[\"train_episodes\"] = train_episodes\n",
    "    \n",
    "    # prepare folders to store results\n",
    "    work_dir = cur_dir/'results'/cfg[\"env_name\"]/str(algo)\n",
    "    model_dir=work_dir/\"model\"\n",
    "    logging_dir=work_dir/\"logging\"\n",
    "    video_train_dir=work_dir/\"video\"/\"train\"\n",
    "    video_test_dir=work_dir/\"video\"/\"test\"\n",
    "    for dir in [work_dir, model_dir, logging_dir, video_train_dir, video_test_dir]:\n",
    "        cu.make_dir(dir)\n",
    "        \n",
    "    cfg.update({'work_dir':work_dir, \"model_dir\":model_dir, \"logging_dir\": logging_dir, \"video_train_dir\": video_train_dir, \"video_test_dir\": video_test_dir})\n",
    "    cfg = SN(**cfg)\n",
    "    \n",
    "    # set seed\n",
    "    if cfg.seed == None:\n",
    "        seed = np.random.randint(low=1, high=1000)\n",
    "    else:\n",
    "        seed = cfg.seed\n",
    "    \n",
    "    ## Create environment\n",
    "    env=cu.create_env(cfg_path /'envs'/f'{env}_env.yaml')\n",
    "\n",
    "   \n",
    "    if cfg.save_video:\n",
    "        # During testing, save every episode\n",
    "        if cfg.testing:\n",
    "            ep_trigger = 1\n",
    "            video_path = cfg.video_test_dir\n",
    "        # During training, save every 50th episode\n",
    "        else:\n",
    "            ep_trigger = 1000   # Save video every 50 episodes\n",
    "            video_path = cfg.video_train_dir\n",
    "        \n",
    "        if render:\n",
    "            env = RecordVideo(\n",
    "                env, video_path,\n",
    "                episode_trigger=lambda x: x % ep_trigger == 0,\n",
    "                name_prefix=cfg.exp_name)\n",
    "\n",
    "\n",
    "    eval_env=copy.deepcopy(env)\n",
    "    env.reset(seed=seed) # we only set the seed here. During training, we don't have to set the seed when performing reset().\n",
    "    eval_env.reset(seed=seed+1000)\n",
    "    eval_env=None # For simplicity, we don't evaluate the performance during training.\n",
    "        \n",
    "    # Get dimensionalities of actions and observations\n",
    "    action_space_dim = cu.get_space_dim(env.action_space)\n",
    "    observation_space_dim = cu.get_space_dim(env.observation_space)\n",
    "    \n",
    "    config={\n",
    "        \"args\": cfg,\n",
    "        \"env\":env,\n",
    "        \"eval_env\":eval_env,\n",
    "        \"action_space_dim\": action_space_dim,\n",
    "        \"observation_space_dim\": observation_space_dim,\n",
    "        \"seed\":seed\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143e0a",
   "metadata": {},
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1</b> (30 points) </h3> \n",
    "    Implement the basic PPO or DDPG algorithm. Modify either the file 'ddpg_agent.py' or 'ppo_agent.py' in the folder 'algos'. \n",
    "    \n",
    "    1. Choose PPO or DDPG\n",
    "    \n",
    "    2. You can check our provided base's mean performance to check if your implementation's performance is acceptable. \n",
    "    \n",
    "    2. Please document your results here, including the training plots and test performance.\n",
    "    \n",
    "    3. Adhere to the given structure to facilitate testing with 'setup' and 'test'  function.\n",
    "    \n",
    "    4. Run the algorithm in the 'easy' and 'middle' environments.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1c0a7",
   "metadata": {},
   "source": [
    "## Task 1.1: Train each agents' performance \n",
    "\n",
    "- Implement your algorithm (either DDPG or PPO) in algos/ddpg_agent.py or algos/ppo_agent.py\n",
    "- After the implementation, train your algorithm with the following code\n",
    "    - Train the algorithm in the first two environments\n",
    "    - The code will train the algorithm with 3 random seeds\n",
    "- Your code must be compatible with the following provided python code\n",
    "\n",
    "**Below, you will find an example of how to test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214060a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This part is provided only for debugging\n",
    "if skip_training == False:\n",
    "    train_episodes = 1000  # Limit the number of training episode for a fast test\n",
    "    \n",
    "    config=setup(algo=algorithm_implemented, env='easy', train_episodes=train_episodes, render=False)\n",
    "    \n",
    "    config[\"seed\"] = 43\n",
    "    \n",
    "    \n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "    # Train the agent using the selected algorithm    \n",
    "    agent.train()\n",
    "    test(agent, 'easy', 'ddpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0bb92",
   "metadata": {},
   "source": [
    "**If everything is fine, we start the proper training now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed52e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "\n",
    "if skip_training == False:\n",
    "    # Choose either PPO or DDPG\n",
    "    implemented_algo = algorithm_implemented #'ppo' or 'ddpg'\n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    environment = 'easy'\n",
    "    training_seeds = []\n",
    "        \n",
    "    # Train the algorithm with a specific random seed.\n",
    "    # In total, we train the algorithm with three random seeds [0, 1, 2].\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=True)\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Train the agent using selected algorithm    \n",
    "        agent.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e073c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "\n",
    "if skip_training == False:\n",
    "    # Choose either PPO or DDPG\n",
    "    implemented_algo = algorithm_implemented #'ppo' or 'ddpg'\n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    environment = 'middle'\n",
    "    training_seeds = []\n",
    "    \n",
    "    # Train the algorithm with a specific random seed.\n",
    "    # In total, we train the algorithm with three random seeds [0, 1, 2].\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment,render=True)\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Train the agent using selected algorithm    \n",
    "        agent.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f156a0a",
   "metadata": {},
   "source": [
    "## Task 1.2: Evaluate the Performance of Each Agent\n",
    "\n",
    "For each environment, the algorithm has been trained using three different random seeds, resulting in the generation of three distinct models for each algorithm. Our next step is to assess the performance of each model.\n",
    "\n",
    "- Execute the code below and document the performance of each model:\n",
    "\n",
    "- Report the mean and standard deviation of the performance across the three random seeds.\n",
    " \n",
    "- Use the provided report format below, and input the values based on the results of your experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code block for training and testing an agent using the implemented algorithm\n",
    "## in the three different Tasks with different difficulty levels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if skip_training == False:\n",
    "    # NOTE: Uncomment the algorithm you implemented\n",
    "    implemented_algo = algorithm_implemented #'ddpg' or 'ppo'\n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    for environment in ['easy', 'middle']:\n",
    "    \n",
    "        training_seeds = []\n",
    "        \n",
    "        # for each algorithm, we will test the agent trained with specific random seed\n",
    "        for i in range(3):\n",
    "            config=setup(algo=implemented_algo, env=environment)\n",
    "    \n",
    "            config[\"seed\"] = i\n",
    "            training_seeds.append(i)\n",
    "    \n",
    "            if config[\"args\"].algo_name == 'ppo':\n",
    "                agent=PPOAgent(config)\n",
    "            elif config[\"args\"].algo_name == 'ddpg':\n",
    "                agent=DDPGAgent(config)\n",
    "            else:\n",
    "                raise Exception('Please use ppo or ddpg!')\n",
    "            \n",
    "            print('\\n\\n\\nnow start testing for environment',environment,' agent:',implemented_algo,' seed:',i)\n",
    "            # Test the agent in the selected environment\n",
    "            test(agent, environment, implemented_algo)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e4f03",
   "metadata": {},
   "source": [
    "If you are curious about visualizing policy behaviours, you can run the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# The example of visualizing the saved test GIFs\n",
    "from IPython.display import display, Image\n",
    "if skip_training == False:\n",
    "# Display the GIF in Jupyter\n",
    "    display(Image(filename=\"imgs/difficult_env.gif\"))  # Change the file path to display yours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ea1f3",
   "metadata": {},
   "source": [
    "## Task 1.3: Plot the algorithm's performance in each environment\n",
    "\n",
    "If all above code runs successfully, now we want to make a plot of the algorithm's training performance. You can run the code below to make plots. The training performance will look similar to this:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvEasy.png\" alt=\"PPO Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvMiddle.png\" alt=\"PPO Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Middle</figcaption>\n",
    "  </figure>\n",
    "  <!-- <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ppo_statistical_SandingEnvDifficult.png\" alt=\"PPO Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>PPO Difficult</figcaption>\n",
    "  </figure> -->\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvEasy.png\" alt=\"DDPG Easy Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Easy</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvMiddle.png\" alt=\"DDPG Middle Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Middle</figcaption>\n",
    "  </figure>\n",
    "  <!-- <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/ddpg_statistical_SandingEnvDifficult.png\" alt=\"DDPG Difficult Environment\" width=\"240\"/>\n",
    "    <figcaption>DDPG Difficult</figcaption>\n",
    "  </figure> -->\n",
    "</p>\n",
    "\n",
    "**Note**: You do not need to make the plots look exactly the same as shown above.  The following code generates 3 figures (1 algorithm x 3 environments). Please comment below the algorithm you did not implement.\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **PPO Easy**: \n",
    "  - `results/SandingEnvMiddle/ppo/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **PPO Middle**: \n",
    "  - `results/SandingEnvMiddle/ppo/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "<!-- - **PPO Difficult**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvDifficult.pdf` -->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3828-206c-4492-bea5-124c14469339",
   "metadata": {},
   "source": [
    "\n",
    " or\n",
    " \n",
    "- **DDPG Easy**: \n",
    "  - `results/SandingEnvMiddle/ddpg/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **DDPG Middle**: \n",
    "  - `results/SandingEnvMiddle/ddpg/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "<!-- - **DDPG Difficult**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvDifficult.pdf` -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9279894",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if skip_training == False:\n",
    "    # Uncomment the algorithm you chose \n",
    "    implemented_algo = algorithm_implemented # 'ppo' or 'ddpg'\n",
    "    \n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    for environment in ['easy', 'middle']:\n",
    "    \n",
    "        training_seeds = []\n",
    "        for i in range(3):\n",
    "            config=setup(algo=implemented_algo, env=environment)\n",
    "    \n",
    "            config[\"seed\"] = i\n",
    "            training_seeds.append(i)\n",
    "    \n",
    "    \n",
    "            if config[\"args\"].algo_name == 'ppo':\n",
    "                agent=PPOAgent(config)\n",
    "            elif config[\"args\"].algo_name == 'ddpg':\n",
    "                agent=DDPGAgent(config)\n",
    "            else:\n",
    "                raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # plot the statistical training curves with specific random seeds\n",
    "        cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6663ed6",
   "metadata": {},
   "source": [
    "DOUBLE CLICK HERE TO EDIT, RUN the above code to make training plots for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd27fda",
   "metadata": {},
   "source": [
    "The following cells are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to test a trained policy\n",
    "def grading_test(agent, env_name, algo_name):\n",
    "    # Load model\n",
    "    agent.load_model()\n",
    "    returns = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    \n",
    "    for ep in range(agent.cfg.test_episodes):\n",
    "        seed = rng.integers(low=1, high=1000)\n",
    "        observation, _ = agent.env.reset(seed=int(seed))\n",
    "        test_reward, test_len, done = 0, 0, False\n",
    "        \n",
    "            \n",
    "        while not done and test_len < agent.cfg.max_episode_steps:\n",
    "            action, _ = agent.get_action(observation, evaluation=True)\n",
    "            observation, reward, done, truncated, info = agent.env.step(action.flatten())\n",
    "            test_reward += reward\n",
    "            test_len += 1\n",
    "        returns.append(test_reward)\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbee40d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f25d4d55ddd2b21f96a57c5087374b05",
     "grade": true,
     "grade_id": "cell-95d055501edce335",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "233ca6cb",
   "metadata": {},
   "source": [
    "Compares the performance of your algorithm with the baseline in order to check that there is no statistical significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cc07f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dd97007a85c886356e6661bd3f0220b",
     "grade": true,
     "grade_id": "cell-af4ea8ed49ad7304",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'easy'\n",
    "\n",
    "def test_task1_1():\n",
    "    baseline_test_returns = []\n",
    "\n",
    "    for i in range(3):\n",
    "        config=setup(algo=algorithm_implemented, env=environment, render=False)\n",
    "        config[\"seed\"] = i\n",
    "        if config[\"args\"].algo_name == 'ppo':\n",
    "            agent=PPOAgent(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg':\n",
    "            agent=DDPGAgent(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "        baseline_test_returns.append(np.sum(grading_test(agent, environment, algorithm_implemented))/agent.cfg.test_episodes)\n",
    "\n",
    "    if algorithm_implemented == 'ppo':\n",
    "        ppo_easy_baseline_target = np.array([0.738, 0.79, 0.78, 0.8,  0.798, 0.774, 0.802, 0.754,  0.756,  0.554])\n",
    "        better_performance = np.mean(baseline_test_returns) > np.mean(ppo_easy_baseline_target)\n",
    "        assert not cu.test_significant_difference(ppo_easy_baseline_target, np.array(baseline_test_returns), 0.05) or better_performance\n",
    "    elif algorithm_implemented == 'ddpg':\n",
    "        ddpg_easy_baseline_target =  np.array([ 0.83, 0.792, 0.804,  0.654, 0.562, 0.8, 0.888, 0.714, 0.724, 0.774])\n",
    "        better_performance = np.mean(baseline_test_returns) > np.mean(ddpg_easy_baseline_target)\n",
    "        assert not cu.test_significant_difference(ddpg_easy_baseline_target, np.array(baseline_test_returns), 0.05) or better_performance\n",
    "\n",
    "\n",
    "test_task1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5754f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='T2'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3><b>Student Task 2</b> (40 points)</h3>\n",
    "    Your objective in this task is to enhance the performance of the DDPG/PPO algorithms, taking inspiration from the suggestions provided in <b>Section 5.II</b>. \n",
    "\n",
    "    1. You must elevate the base algorithm's performance to ensure the agent's success in the moderate difficulty environment (environment = 'middle'). \n",
    "    \n",
    "    2. Please document your results here, including the training plots and test performance.\n",
    "    \n",
    "    3. Adhere to the given structure to facilitate testing with 'setup' and 'test'  function.\n",
    "    \n",
    "    4. If you choose PPO, implement either Dual-Clip PPO or Self Imitation Learning (SIL).\n",
    "    \n",
    "    5. If you opt for DDPG, implement either LNSS DDPG or SIL.\n",
    "    \n",
    "    6. Carefully read the extension guidelines outlined in <b>Section 5.II</b>, and proceed to modify either 'ddpg_extension.py' or 'ppo_extension.py' located in the 'algos' folder. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086909da",
   "metadata": {},
   "source": [
    "## Task 2.1: Enhance Your Chosen Algorithm\n",
    "\n",
    "### a) Overview\n",
    "Improve the performance of your selected reinforcement learning algorithm. Ensure that your implementations are properly documented and organized for clarity.\n",
    "\n",
    "### b) Implementation Details\n",
    "- **Algorithm Improvements**: Enhance your chosen algorithm.\n",
    "  - If you choose PPO, implement either Dual-Clip PPO or Self Imitation Learning (SIL)\n",
    "  -  If you opt for DDPG, implement either LNSS  or SIL.\n",
    "  - Ensure that the performance is noticeably improved.\n",
    "  - Place your implementations in the appropriate file:\n",
    "    - 'algos/ddpg_extension.py' for DDPG\n",
    "    - 'algos/ppo_extension.py' for PPO\n",
    "\n",
    "### c) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### d) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **middle-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### e) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5a520",
   "metadata": {},
   "source": [
    "**Train**: After implementing the improvement extensions, run the following code to train your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your improved algorithm either in algos/ddpg_extension.py or algos/ppo_extension.py\n",
    "# uncomment the following line to import the agent\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "# from algos.ppo_extension import PPOExtension\n",
    "\n",
    "implemented_algo = '' # choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'middle'\n",
    "if skip_training == False:\n",
    "    training_seeds = []\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment, render=True)\n",
    "    \n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "    \n",
    "    \n",
    "        if config[\"args\"].algo_name == 'ppo_extension':\n",
    "            agent=PPOExtension(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "            agent=DDPGExtension(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Train the agent using selected algorithm    \n",
    "        agent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0584b",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_training == False:\n",
    "    training_seeds = []\n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment)\n",
    "    \n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "    \n",
    "    \n",
    "        if config[\"args\"].algo_name == 'ppo_extension':\n",
    "            agent=PPOExtension(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "            agent=DDPGExtension(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "    \n",
    "        # Test the agent in the selected environment\n",
    "        test(agent, environment, implemented_algo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c93abe",
   "metadata": {},
   "source": [
    "## Task 2.2: Plot improved algorithm performance \n",
    "\n",
    "### a) Display the plots:\n",
    "Display the training performance of your improved algorithm, similarly as in task 1.3\n",
    "\n",
    "### b) Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **improved Middle**: \n",
    "  - `results/SandingEnvMiddle/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if skip_training == False:\n",
    "    \n",
    "    # Uncomment the algorithm you chose \n",
    "    implemented_algo = # 'ppo_extension' or 'ddpg_extension'\n",
    "    environment = 'middle'\n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    \n",
    "    training_seeds = [0,1,2]\n",
    "    \n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "    \n",
    "    config[\"seed\"] = 0\n",
    "    \n",
    "    agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "    \n",
    "    # plot the statistical training curves with specific random seeds\n",
    "    cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b022732",
   "metadata": {},
   "source": [
    "## Task 2.3: Comparison of Improved and Original Algorithm Performance\n",
    "\n",
    "### a) Display the Plots\n",
    "Display the training performance of both the improved and the original algorithms.\n",
    "\n",
    "We aim to compare the training performances of the original and improved algorithms. To achieve this, we will generate the following plots, which will highlight the sample efficiency and the agent's performance throughout the training process. Below are some figures comparing the performances of DDPG and PPO:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure style=\"display: inline-block; text-align: center; margin: 10px;\">\n",
    "    <img src=\"imgs/middle_compare_ddpg_ppo.png\" alt=\"PPO Middle Environment\" width=\"540\"/>\n",
    "    <figcaption>PPO vs DDPG (Middle environment)</figcaption>\n",
    "  </figure>\n",
    "  \n",
    "</p>\n",
    "\n",
    "**Note**: The display does not need to exactly match the figures shown above. However, the code should generate a figure to compare the original algorithm with the improved algorithm.\n",
    "\n",
    "### b) Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Original vs Improved (Middle Environment)**: \n",
    "  - `results/SandingEnvMiddle/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvMiddle/compare_ppo_ppo_extension.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "environment = 'middle'\n",
    "\n",
    "if skip_training == False:\n",
    "    orgin_alo_name = # 'ddpg' or 'ppo'\n",
    "    improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "    \n",
    "    config=setup(algo=orgin_alo_name, env=environment)\n",
    "    origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "    \n",
    "    config=setup(algo=improved_alo_name, env=environment)\n",
    "    improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "    \n",
    "    # make the comparison plot\n",
    "    cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d196647",
   "metadata": {},
   "source": [
    "The following cells are used for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36df1fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b314d492be78516304cb030379ef961",
     "grade": true,
     "grade_id": "cell-9d8e5e08d1363166",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68020e11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b810d04b7d165966c3a4db8d95dde2fb",
     "grade": true,
     "grade_id": "cell-0bb4ac707a511f13",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8862a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ac107666d3cf2a6f2bd6e955a3c3766",
     "grade": true,
     "grade_id": "cell-5d6cc788ed986ab2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bcd0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b0ecfca3eb6c1e2dd2ee3301c86cc3b",
     "grade": true,
     "grade_id": "cell-b5f5716e20150302",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ff0f5",
   "metadata": {},
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3><b>Student Question 1</b> (30 points)</h3> \n",
    "    \n",
    "    1. Answer the questions regarding how you extended PPO/DDPG. ONLY choose the question group related to your extension.\n",
    "    \n",
    "    2. Each question will tell you the maximum number of answers you can select, however the number of correct answers can be lower.\n",
    "    \n",
    "    3. Please do not select more answers than the maximum number as that will reduce your points to zero.\n",
    "    \n",
    "    4. You get full points for selecting the correct answers in every question.\n",
    "    \n",
    "    5. You get partial points if you select a mix of correct and wrong answers.\n",
    "    \n",
    "    6. Make sure to submit your answers.\n",
    "    \n",
    "    7. For LNSS questions you may seek help from https://openreview.net/forum?id=EGfYnTyEGv\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d572c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Define the questions for each topic with the exact content you provided\n",
    "ppo_questions = [\n",
    "    {\n",
    "        'question_id': 'PPO_Q1',\n",
    "        'question': 'What problem does Dual-Clip PPO aim to address? (Select a maximum of 3 options)',\n",
    "        'options': {\n",
    "            'A': 'Reducing computational complexity',\n",
    "            'B': 'Managing large policy deviations',\n",
    "            'C': 'Enhancing exploration',\n",
    "            'D': 'Bounding the objective when advantage is less than zero'\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'PPO_Q2',\n",
    "        'question': 'How does Dual-Clip PPO differ from standard PPO in its clipping strategy? (Select a maximum of 3 options)',\n",
    "        'options': {\n",
    "            'A': 'It uses a single clipping parameter',\n",
    "            'B': 'It introduces a second clipping for negative advantages',\n",
    "            'C': 'The range of ratio in PPO is $(1- \\\\epsilon, \\\\infty)$, while in Dual-PPO is $(1- \\\\epsilon, c)$ when advantage is less than zero',\n",
    "            'D': 'It clips both state and action spaces'\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "sil_questions = [\n",
    "    {\n",
    "        'question_id': 'SIL_Q1',\n",
    "        'question': 'Which of the following describes how SIL selects experiences for learning? (Select a maximum of 3 options)',\n",
    "        'options': {\n",
    "            'A': 'SIL uses experiences that maximize the entropy of the policy.',\n",
    "            'B': 'SIL uses experiences where the past returns exceed the agent\\'s current value estimate.',\n",
    "            'C': 'SIL prioritizes experiences based on clipped advantage.',\n",
    "            'D': 'SIL selects random experiences from the replay buffer.'\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'SIL_Q2',\n",
    "        'question': 'What are the potential downsides of using SIL? (Select a maximum of 3 options)',\n",
    "        'options': {\n",
    "            'A': 'It may lead to overfitting on suboptimal past experiences.',\n",
    "            'B': 'It can significantly increase the computational load.',\n",
    "            'C': 'It requires a large amount of expert data to start learning.',\n",
    "            'D': 'Some off-policy methods may not benefit much from SIL.'\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "ddpg_lnss_questions = [\n",
    "    {\n",
    "        'question_id': 'DDPG_Q1',\n",
    "        'question': 'What are the correct descriptions of LNSS: (Select a maximum of 5 options)',\n",
    "        'options': {\n",
    "            'A': 'The surrogate reward actually is a weighted average of discounted N-step return;',\n",
    "            'B': (\n",
    "                'Given following equations\\n'\n",
    "                '$$G_k = \\\\sum_{t=k}^{k+N-1} \\\\gamma^{t-k} r_t$$,\\n'\n",
    "                '$$ r\\'_k = \\\\frac{\\\\sum_{t=k}^{k+N-1} \\\\gamma^{t-k} r_t}{\\\\sum_{n=0}^{N-1} \\\\gamma^n}$$,\\n'\n",
    "                '$$ r\\'_k = \\\\frac{\\\\gamma - 1}{\\\\gamma^N - 1} \\\\sum_{t=k}^{k+N-1} \\\\gamma^{t-k} r_t.$$ \\n'\n",
    "                'These three equations describe different accumulated reward for making a surrogate reward.'\n",
    "            ),\n",
    "            'C': 'LNSS can reduce the estimated Q-value variance ;',\n",
    "            'D': 'Use a larger $N$ hyperparameter for calculating the surrogate reward can help reduce the upper bound of estimated Q-value variance ;',\n",
    "            'E': (\n",
    "                'Given the surrogate reward $r\\'_t$ defined in Eq.7, the accumulated reward $G\\'_k = \\\\sum_{t=k}^{k+N-1} \\\\gamma^{t-k} r\\'_t$, upon learning convergence, '\n",
    "                '$G\\'_k$ is a biased estimate of $G_k$ in Eq.4'\n",
    "            ),\n",
    "            'F': (\n",
    "                'If we assume that the given policy $\\\\pi$ results in the same expectation of stage reward, i.e., $\\\\mathbb{E}^{\\\\pi}[r_k] = \\\\mathbb{E}^{\\\\pi}[r\\'_k]$ for all $k$. '\n",
    "                'then the $Q^{\\\\pi}$ of using the original reward and $Q^{\\\\pi}$ of using LNSS are the same. '\n",
    "            ),\n",
    "            'G': (\n",
    "                'If $\\\\pi$ refers to policy $\\\\pi_j$ for $j \\\\to \\\\infty$ (meaning that episode horizon is infinite). In this case, we can consider '\n",
    "                '$Q^{\\\\pi} = Q^{\\\\pi_{\\\\infty}}, \\\\mathbf{Q}^{\\\\bar{\\\\pi}} = \\\\mathbf{Q}^{\\\\bar{\\\\pi}_{\\\\infty}}$ but  $Q^{\\\\pi_{\\\\infty}} \\\\neq \\\\mathbf{Q}^{\\\\bar{\\\\pi}_{\\\\infty}}$.\\n'\n",
    "                'This is because using different rewards, the target policy will be different during policy improvement so that it results in different $Q$ values.'\n",
    "            ),\n",
    "            \n",
    "            \n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'DDPG_Q2',\n",
    "        'question': '“n-step” RL and LNSS: (Select a maximum of 4 options)',\n",
    "        'options': {\n",
    "            'A': (\n",
    "                'LNSS can have unbiased estimation of Q-value, but “n-step” RL like n-step DDPG or n-step D4PG with deterministic policy may not. '\n",
    "            ),\n",
    "            'B': (\n",
    "                'Off-policy n-step methods such as TD($\\\\lambda$) and Q($\\\\sigma$) are shown to decrease the estimation bias also increases the variance. '\n",
    "                'As LNSS aims to, and is shown to, reduce variance without sacrificing bias.'\n",
    "            ),\n",
    "            'C': 'Importance sampling is required when applying n-step bootstrapping to calculate the TD target when the policy is non-deterministic   ',\n",
    "            'D': 'If we use LNSS, we should better not to use “n-step” RL ',\n",
    "            'E': (\n",
    "                'LNSS theoretically reduces the upper bound of variance according to Theorem 1. Furthermore, this bound linearly decreases as N increases.  '\n",
    "            ),\n",
    "            'F': 'Using the LNSS surrogate reward $r’_k$ may introduce additional estimation bias ',\n",
    "            'G': (\n",
    "                'Off-policy n-step methods such as TD($\\\\lambda$) and Q($\\\\sigma$) are shown to increase the estimation bias also increases the variance. '\n",
    "                'As LNSS aims to, and is shown to, reduce variance without sacrificing bias. '\n",
    "            ),\n",
    "            'H': 'LNSS is more like automatically design a dense reward but it is not for “n-step” RL.',\n",
    "\n",
    "            'I': '''For classical Q learning, we can directly use this equation to calculate the n-step TD target to update the Q function\n",
    "$$\n",
    "\\\\sum_{t=k}^{k+n-1} \\\\gamma^{t-k} r_t + \\\\gamma^n Q^{\\\\pi}(s_{k+n}, a_{k+n})\n",
    "$$.\n",
    "'''\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'question_id': 'DDPG_Q3',\n",
    "        'question': 'What are the correct descriptions of LNSS implementations: (Select a maximum of 3 options)',\n",
    "        'options': {\n",
    "            'A': 'We can randomly define N as we want as long as $N \\\\leq T$ where $T$ is the task horizon.',\n",
    "            'B': (\n",
    "                'Because LNSS needs to maintain another additional buffer to store the reward, so it doubles the memory for saving data during training. '\n",
    "            ),\n",
    "            'C': (\n",
    "                'In some cases, LNSS will not relabel each interaction by recalculate and relabel reward of the transition tuple $(s, a, r, s’)$  ->  $(s, a, r_{LNSS}, s’)$.\\n'\n",
    "                'For example,  $k+N-1$ exceed the episode horizon then we will not recalculate the reward.'\n",
    "            ),\n",
    "            'D': (\n",
    "                'If $k + N - 1 \\\\geq T$, which signifies a sufficient number of experience samples to compute LNSS surrogate reward $r\\'$ with full $N$ steps, then\\n'\n",
    "                '$$ r\\' = \\\\left( \\\\sum_{t=k}^{k+N-1} \\\\gamma^{t-k} r_t \\\\right) * \\\\frac{\\\\gamma - 1}{\\\\gamma^N - 1}. $$  '\n",
    "            ),\n",
    "            'E': 'Compared with classical off-policy RLs (DDPG, TD3, SAC, etc, ...) replay buffer, LNSS needs to maintain another additional buffer to store experience.',\n",
    "            'F': (\n",
    "                'In LNSS, we should not only store the re-calculated dense reward, but also need to keep the original reward in experience replay buffer.'\n",
    "            )\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to create question widgets with left-aligned options\n",
    "def create_question_widget(question_data):\n",
    "    question_id = question_data['question_id']\n",
    "    question_text = question_data['question']\n",
    "    options = question_data['options']\n",
    "    \n",
    "    # Display the question\n",
    "    display(Markdown(f\"**{question_id}. {question_text}**\"))\n",
    "    \n",
    "    # Create checkboxes for options\n",
    "    option_widgets = []\n",
    "    for key, value in options.items():\n",
    "        if value.strip() == '':\n",
    "            continue  # Skip empty options\n",
    "        cb = widgets.Checkbox(value=False, layout=widgets.Layout(width='auto'))\n",
    "        # Use HTMLMath widget to display LaTeX equations\n",
    "        label_html = widgets.HTMLMath(value=f\"<b>{key})</b> {value}\", layout=widgets.Layout(width='auto'))\n",
    "        # Combine checkbox and label, align items to the start (left)\n",
    "        hb = widgets.HBox([cb, label_html], layout=widgets.Layout(align_items='flex-start'))\n",
    "        option_widgets.append({'option_letter': key, 'checkbox': cb, 'widget': hb})\n",
    "    \n",
    "    # Arrange options vertically, align items to the start (left)\n",
    "    options_box = widgets.VBox([item['widget'] for item in option_widgets], layout=widgets.Layout(align_items='flex-start'))\n",
    "    display(options_box)\n",
    "    \n",
    "    # Return the list of option widgets so we can retrieve the values later\n",
    "    return {'question_id': question_id, 'option_widgets': option_widgets}\n",
    "\n",
    "# Create topic selector\n",
    "topic_selector = widgets.SelectMultiple(\n",
    "    options=['PPO', 'SIL', 'DDPG LNSS'],\n",
    "    description='Select Topic(s):',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "proceed_button = widgets.Button(description=\"Proceed\")\n",
    "display(topic_selector, proceed_button)\n",
    "\n",
    "display_area = widgets.Output()\n",
    "display(display_area)\n",
    "\n",
    "# Global variable to store all question widgets\n",
    "all_question_widgets = []\n",
    "\n",
    "# Function to handle proceed button click\n",
    "def on_proceed_button_clicked(b):\n",
    "    # Clear previous output\n",
    "    display_area.clear_output()\n",
    "    selected_topics = topic_selector.value\n",
    "    # Clear previous question widgets\n",
    "    global all_question_widgets\n",
    "    all_question_widgets = []\n",
    "    with display_area:\n",
    "        for topic in selected_topics:\n",
    "            if topic == 'PPO':\n",
    "                for question_data in ppo_questions:\n",
    "                    question_widget = create_question_widget(question_data)\n",
    "                    all_question_widgets.append(question_widget)\n",
    "            elif topic == 'SIL':\n",
    "                for question_data in sil_questions:\n",
    "                    question_widget = create_question_widget(question_data)\n",
    "                    all_question_widgets.append(question_widget)\n",
    "            elif topic == 'DDPG LNSS':\n",
    "                for question_data in ddpg_lnss_questions:\n",
    "                    question_widget = create_question_widget(question_data)\n",
    "                    all_question_widgets.append(question_widget)\n",
    "        # Display the submit button\n",
    "        display(submit_button, output)\n",
    "\n",
    "proceed_button.on_click(on_proceed_button_clicked)\n",
    "\n",
    "# Create submit button and output area\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "output = widgets.Output()\n",
    "submission = {}\n",
    "# Function to handle submit button click\n",
    "def on_submit_button_clicked(b):\n",
    "    with output:\n",
    "        # Clear previous output\n",
    "        output.clear_output()\n",
    "        # Collect the answers\n",
    "        print(\"Your submission:\")\n",
    "        \n",
    "        # Create a mapping from question_id to topic\n",
    "        question_id_to_topic = {}\n",
    "        # Build the mapping\n",
    "        for topic in ['PPO', 'SIL', 'DDPG LNSS']:\n",
    "            if topic == 'PPO':\n",
    "                questions = ppo_questions\n",
    "            elif topic == 'SIL':\n",
    "                questions = sil_questions\n",
    "            elif topic == 'DDPG LNSS':\n",
    "                questions = ddpg_lnss_questions\n",
    "            for question_data in questions:\n",
    "                question_id = question_data['question_id']\n",
    "                question_id_to_topic[question_id] = topic\n",
    "        \n",
    "        # Prepare the submission variable\n",
    "        global submission  # Declare submission as global to modify it inside the function\n",
    "        submission = {}\n",
    "        # Now process the submissions\n",
    "        for question in all_question_widgets:\n",
    "            question_id = question['question_id']\n",
    "            option_widgets = question['option_widgets']\n",
    "            selected_options = []\n",
    "            for item in option_widgets:\n",
    "                if item['checkbox'].value:\n",
    "                    selected_options.append(item['option_letter'])\n",
    "            # Get the topic from the mapping\n",
    "            question_topic = question_id_to_topic.get(question_id, 'Unknown Topic')\n",
    "            # Store the submission data\n",
    "            \n",
    "            submission[question_id] = {\n",
    "                'selected_options': selected_options\n",
    "            }\n",
    "            # Print the submission for each question including the topic\n",
    "            print(f\"{question_topic} - {question_id}: {''.join(selected_options)}\")\n",
    "        \n",
    "        # After processing all submissions, print the submission variable\n",
    "        print(\"\\nSubmission variable contents:\")\n",
    "        print(submission)\n",
    "        with open('results/answers.pkl', 'wb') as f:\n",
    "            pickle.dump(submission, f)\n",
    "\n",
    "\n",
    "# Assign the event handler to the submit button\n",
    "submit_button.on_click(on_submit_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d34f81",
   "metadata": {},
   "source": [
    "The following cells are used for grading, please do not remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0630b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bfa961e2a588f67cfeebe96871a8540",
     "grade": true,
     "grade_id": "cell-1593512b55e67afd",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a241bfe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b89f69e6990f7c92f65777c32d16d26",
     "grade": true,
     "grade_id": "cell-4841cbd3afab6a02",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6855d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a26d6d16ba9af5a2303e44ad7f96cbb6",
     "grade": true,
     "grade_id": "cell-8bec6f39f969e5cc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679a0bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0ebade504c3886ef9d4b10fa2749a9b",
     "grade": true,
     "grade_id": "cell-67df27f7c45d9a6c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33f383",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ab8e2ebeaa6d58096c9c6a56e6d860c",
     "grade": true,
     "grade_id": "cell-b2e820db12109925",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cf0e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80485c4b97571b9cb784ac7e8801a47e",
     "grade": true,
     "grade_id": "cell-fa40bf8988a8717c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb364a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b2ba2f84dec0f966c296ca2a3e18aa0",
     "grade": true,
     "grade_id": "cell-d00655784143f6ee",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0956e37e",
   "metadata": {},
   "source": [
    "<a id='T3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 3</b> (+20 points) </h3>\n",
    "    This task gives bonus points to the project works that get highest performance. \n",
    "    \n",
    "    1. Your projects shall be evaluated based on their performance in the difficult environment.\n",
    "    \n",
    "    2. Among all submissions, the best performing project (100% ranked) will receive 20 bonus points, 95% ranked or above will get 10 bonus points. (+20 points) \n",
    "    \n",
    "    3. Run the cell below to do the evaluation, make sure all the generated files are included when you submit your work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50416d1f",
   "metadata": {},
   "source": [
    "## Task 3.1: Evaluate Your Improved Algorithm with difficult environment\n",
    "\n",
    "\n",
    "### a) Training\n",
    "- **Random Seeds**: Train your algorithm using three distinct random seeds [0,1,2] to ensure robustness and repeatability.\n",
    "\n",
    "### b) Evaluation\n",
    "- **Environment**: Evaluate your algorithm exclusively in the **difficult-level difficulty environment** to focus your improvements.\n",
    "\n",
    "### c) Code Compatibility\n",
    "- Ensure that your code is **fully compatible** with all existing functions in other files, maintaining the integrity of the overall project structure.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f06c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from algos.ddpg_agent import DDPGAgent\n",
    "from algos.ppo_agent import PPOAgent\n",
    "from algos.ddpg_extension import DDPGExtension\n",
    "from algos.ppo_extension import PPOExtension\n",
    "# implement your improved algorithm either in algos/ddpg_extension.py or algos/ppo_extension.py\n",
    "\n",
    "implemented_algo = ''# choose 'ppo_extension' or 'ddpg_extension'\n",
    "environment = 'difficult'\n",
    "\n",
    "\n",
    "training_seeds = []\n",
    "for i in range(3):\n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "    config[\"seed\"] = i\n",
    "    training_seeds.append(i)\n",
    "\n",
    "\n",
    "    if config[\"args\"].algo_name == 'ppo':\n",
    "        agent=PPOAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg':\n",
    "        agent=DDPGAgent(config)\n",
    "    elif config[\"args\"].algo_name == 'ppo_extension':\n",
    "        agent=PPOExtension(config)\n",
    "    elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "        agent=DDPGExtension(config)\n",
    "    else:\n",
    "        raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "    # Train the agent using selected algorithm    \n",
    "    agent.train()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379623a",
   "metadata": {},
   "source": [
    "**Test**: After training, run the following code to test your agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if skip_training == False:\n",
    "    training_seeds = []\n",
    "    difficult_test_returns = [] \n",
    "    for i in range(3):\n",
    "        config=setup(algo=implemented_algo, env=environment)\n",
    "\n",
    "        config[\"seed\"] = i\n",
    "        training_seeds.append(i)\n",
    "\n",
    "\n",
    "        if config[\"args\"].algo_name == 'ppo_extension':\n",
    "            agent=PPOExtension(config)\n",
    "        elif config[\"args\"].algo_name == 'ddpg_extension':\n",
    "            agent=DDPGExtension(config)\n",
    "        else:\n",
    "            raise Exception('Please use ppo or ddpg!')\n",
    "\n",
    "        # Test the agent in the selected environment\n",
    "        difficult_test_returns.append(np.mean(test(agent, environment, implemented_algo))/agent.cfg.test_episodes)\n",
    "\n",
    "    with open('results/competition_returns.pkl', 'wb') as f:\n",
    "        pickle.dump(difficult_test_returns, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a05f0f",
   "metadata": {},
   "source": [
    "Make sure the \"competition_returns.pkl\" file is saved in your results directory. We will use this file to compare performances for the competition and we will retest the best performing agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9da1f8",
   "metadata": {},
   "source": [
    "## Task 3.2: Plot the Improved Algorithm's Performance \n",
    "\n",
    "#### Display the Plots\n",
    "Display the training performance of your improved algorithm, similar to what was done in Task 2.2.\n",
    "\n",
    "#### Paths\n",
    "If the code runs successfully, your plot should be saved to the following paths:\n",
    "\n",
    "- **Improved Difficult**: \n",
    "  - `results/SandingEnvDifficult/ppo_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n",
    "  \n",
    "  or\n",
    "  \n",
    "  - `results/SandingEnvDifficult/ddpg_extension/logging/figure_statistical_SandingEnvDifficult.pdf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f83a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to plot PPO or DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if skip_training == False:\n",
    "    # Uncomment the algorithm you chose \n",
    "    implemented_algo = # 'ppo_extension' or 'ddpg_extension'\n",
    "    environment = 'difficult'\n",
    "    \n",
    "    # Loop over the three difficulty levels\n",
    "    \n",
    "    training_seeds = [0,1,2]\n",
    "    \n",
    "    config=setup(algo=implemented_algo, env=environment)\n",
    "    \n",
    "    config[\"seed\"] = 0\n",
    "    \n",
    "    agent=# DDPGExtension(config) or PPOExtension(config)\n",
    "    \n",
    "    # plot the statistical training curves with specific random seeds\n",
    "    cu.plot_algorithm_training(agent.logging_dir, training_seeds, agent.env_name, implemented_algo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd319d",
   "metadata": {},
   "source": [
    "## Task 3.3: Plot improved algorithm's and original's comparison performance\n",
    "\n",
    "### Display the plots:\n",
    "Display the training performance of your improvement algorithm, similarly as in task 2.3\n",
    "\n",
    "### Paths:\n",
    "Your plot should be plotted in the following paths if the code runs successfully:\n",
    "\n",
    "- **Original vs Improved (difficult environment)**: \n",
    "  - `results/SandingEnvDifficult/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvDifficult/compare_ppo_ppo_extension.pdf`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following code to draw the comparison plots of PPO and DDPG's training performances\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if skip_training == False:\n",
    "    environment = 'difficult'\n",
    "\n",
    "    orgin_algo_name = # 'ddpg' or 'ppo'\n",
    "    improved_alo_name = # 'ddpg_extension' or 'ppo_extension'\n",
    "\n",
    "    config=setup(algo=orgin_algo_name, env=environment)\n",
    "    origin_agent = # DDPGAgent(config) or PPOAgent(config)\n",
    "\n",
    "    config=setup(algo=improved_alo_name, env=environment)\n",
    "    improved_agent = # DDPGExtension(config) or PPOExtension(config)\n",
    "\n",
    "    # make the comparison plot\n",
    "    cu.compare_algorithm_training(origin_agent, improved_agent, seeds=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da348f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d871ab6a963c56f70ff92b6dd84f4148",
     "grade": true,
     "grade_id": "cell-0d5848309f9ed3ff",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True\n",
    "assert algorithm_implemented in ['ppo', 'ddpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb02ea-e912-46fa-9d35-e4869b07c28b",
   "metadata": {
    "tags": []
   },
   "source": [
    " ---\n",
    "## 9 Submitting <a id='9.'></a>\n",
    "Ensure that all tasks and questions are answered, and that the necessary plots are saved in the appropriate locations. Below is a list of the relevant plots and files that need to be submitted for the project work:\n",
    "\n",
    "### 1. Model Weights\n",
    "For each algorithm, you should have saved three model weights in the corresponding paths:\n",
    "\n",
    "`'results/(environment name)/(algorithm name)/model/model_parameters_(seed number).pt'`\n",
    "\n",
    "**Examples:**\n",
    "- For a DDPG agent trained with seed number 0 in the middle-level sanding environment:\n",
    "  - `'results/SandingEnvMiddle/ddpg/model/model_parameters_0.pt'`\n",
    "\n",
    "**Submission Checklist:**\n",
    "Ensure that each algorithm (ddpg, ppo, ddpg_extension, ppo_extension) has three sets of model weights (model_parameters_0, model_parameters_1, model_parameters_2) saved in the above paths.\n",
    "\n",
    "### 2. Individual Algorithm Plots\n",
    "\n",
    "You need to check that you have plotted the average training performances and the comparison plots for each algorithm. For submission, ensure the following figures are included:\n",
    "\n",
    "#### Task 1.3:\n",
    "\n",
    "- **PPO Easy**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **PPO Middle**: \n",
    "  - `results/SandingEnvMiddle/PPO/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "  \n",
    "  or\n",
    "  \n",
    "- **DDPG Easy**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvEasy.pdf`\n",
    "- **DDPG Middle**: \n",
    "  - `results/SandingEnvMiddle/DDPG/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n",
    "\n",
    "#### Task 2.3:\n",
    "\n",
    "- **Improved agent for Middle-level environment**: \n",
    "  - `results/SandingEnvMiddle/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n",
    "#### Task 2.4:\n",
    "\n",
    "- **Original vs Improved (Middle-level Environment)**: \n",
    "  - `results/SandingEnvMiddle/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvMiddle/compare_ppo_ppo_extension.pdf`\n",
    "  \n",
    "#### Task 3.3:\n",
    "\n",
    "- **Improved Difficult-level environment**: \n",
    "  - `results/SandingEnvDifficult/ppo_extension(or ddpg_extension)/logging/figure_statistical_SandingEnvMiddle.pdf`\n",
    "\n",
    "#### Task 3.4:\n",
    "\n",
    "- **Original vs Improved (Difficult-level Environment)**: \n",
    "  - `results/SandingEnvDifficult/compare_ddpg_ddpg_extension.pdf`\n",
    "  - or \n",
    "  - `results/SandingEnvDifficult/compare_ppo_ppo_extension.pdf`\n",
    "\n",
    "### 3. Student question 1 answers \n",
    "  Ensure that you have saved your answers to the student question 1 in the file:\n",
    "    `results/answers.pkl`\n",
    "\n",
    "### 4. Competition results\n",
    "  The returns of the extension agent on the difficult enviroment should be saved in the file:\n",
    "    `results/competition_returns.pkl` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a07d0e-2b83-4e02-8e9b-c457fbb35485",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Feedback <a id='10.'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58b944-9121-4827-a042-72b01ed21146",
   "metadata": {},
   "source": [
    "1) How much time did the project work members in total spend on the project work? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 35.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f3e51-b088-4c7d-8ad4-4eac14e6691c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f72584-ec47-492f-8d91-beabc1bbf10d",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a68775-ff72-40c5-b29b-51de6e6df98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing PPO or DDPG for the sanding task (35 points)\n",
    "T2 = None   # Extending PPO/DDPG to work on the easy and moderate difficulty tasks (45 points)\n",
    "T3 = None   # Extending PPO/DDPG to work on the difficult task (+20 points)\n",
    "Q1 = None   # Question 1 How did you extend PPO/DDPG and why? (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fb542-b18b-453a-8ce2-b3bd72b43b4a",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668114c2-b143-4971-9f3f-dc18583aef8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing PPO or DDPG for the sanding task (35 points)\n",
    "T2 = None   # Extending PPO/DDPG to work on the easy and moderate difficulty tasks (45 points)\n",
    "T3 = None   # Extending PPO/DDPG to work on the difficult task (+20 points)\n",
    "Q1 = None   # Question 1 How did you extend PPO/DDPG and why? (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07196-619f-47d7-85be-6d2c2d5334d8",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - How difficult / time consuming was the project work? What was the most difficult part?\n",
    "    - What should be changed in the project work?\n",
    "    - What was the most useful / interesting part in the project work?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994fbd2-12d1-41ef-aa8f-16e4cc266fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
