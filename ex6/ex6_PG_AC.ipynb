{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8323a86e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "968ecb4cb3b8623792cd5938edc4ea1b",
     "grade": false,
     "grade_id": "cell-47bfc0ea1447f571",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 6 - Actor Critic part 1 </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is part of the teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Policy Gradient with a Critic </a>\n",
    "* <a href='#3.'> 3. Submitting </a>\n",
    "* <a href='#3.1'> 3.1 Feedback </a>\n",
    "* <a href='#4.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing PG with critic (20 points)</a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> Relationship between actor-critic and REINFORCE with baseline (10 points)</a>\\\n",
    "<a href='#Q2'><b>Student Question 1.2</b> Advantage (5 points)  </a>\\\n",
    "<a href='#Q3'><b>Student Question 1.3</b> Bias and Variance Analysis (10 points) </a>\\\n",
    "<a href='#Q4'><b>Student Question 1.4</b> Controlling bias-variance tradeoff (10 points)</a>\n",
    "    \n",
    "**Total Points:** 55\n",
    "\n",
    "**Estimated runtime of all the cells:** 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53631b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4ce01f58b207f5715f395b180619b9e",
     "grade": false,
     "grade_id": "cell-6ef87fd4c2d2cc44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "\n",
    "In this assignment, we will implement an actor-critic reinforcement learning algorithm which combines elements of both value-based methods (critic) and policy-based methods (actor) to improve learning and stability in the **InvertedPendulum-v4**  environment.\n",
    "\n",
    "## 1.1 Task environments: <a id='1.1'></a>\n",
    "\n",
    "In this exercise, we will focus on InvertedPendulum-v4 tasks:\n",
    "- InvertedPendulum-v4(https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/): This environment is similar to the cartpole environment but now powered by the Mujoco physics simulator - allowing for more complex experiments (such as varying the effects of gravity). This environment involves a cart that moves horizontally, with a pole fixed on the cart at one end and the other end of the pole moving freely. The cart can be pushed left or right. The goal is to move the pole such that it is vertically above the cart pointing straight up by applying horizontal forces on the cart.\n",
    "<figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/InvertedPendulum.png\" width=\"300\"/>\n",
    "    <figcaption style=\"text-align: center\">  Figure 1: The InvertedPendulum-v4 environment. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## 1.2 Learning Objectives: <a id='1.1'></a>\n",
    "\n",
    "- Understand the idea of actor-critic algorithms\n",
    "- Understand the limits and use cases of actor-critics\n",
    "\n",
    "## 1.3 Code Structure & Files <a id='1.2'></a>\n",
    "\n",
    "```ex6_PG_AC.ipynb``` is the file needed to be modified for this part of the assignment.  \n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER PART IN ```ex6_DDPG.ipynb```** </span>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                            # Config files for environments\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                           # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄHalfCheetah-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*HalfCheetah-v4_params.pt    # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*ddpg.png               # Contains training performance plot\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄInvertedPendulum-v4\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging                \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄlogging.pkl        # Contains logged data\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel              \n",
    "‚îÇ   ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ*InvertedPendulum-v4_params.pt      # Contains trained model\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo                   # Videos saved\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ*pg_ac.png              # Contains training performance plot\n",
    "‚îÇ   ex6_DDPG.ipynb                  # 2nd assignment file containing tasks <---------\n",
    "‚îÇ   ex6_PG_AC.ipynb                 # 1st assignment file containing tasks <---------This task\n",
    "‚îÇ   train.py                        # Contains train and test functions \n",
    "‚îÇ   utils.py                        # Contains useful functions \n",
    "‚îî‚îÄ‚îÄ‚îÄbuffer.py                       # Contains buffer functions\n",
    "```\n",
    "\n",
    "## 1.4 Execution time <a id='1.4'></a>\n",
    "\n",
    "The training of Actor-Critic may take more than 30 mins depending on the server load. If you have problems with the training time, you can train locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef1c90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2a4fdb4406c82cbf0924a4ae9fd56ca",
     "grade": false,
     "grade_id": "cell-76eb1e3dcf217198",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79df48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd4a8717cd49c894ce9a4aa3a1895148",
     "grade": false,
     "grade_id": "cell-78fdf22a73920722",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# 2. Policy Gradient with a Critic <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584d11d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0439bebaa3d0d286f254868ab55e9939",
     "grade": false,
     "grade_id": "cell-61899a5dd880417b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implement policy gradient (PG) with critic (20 points) </h3> \n",
    "\n",
    "Revisit the policy gradient solution for the InvertedPendulum from Exercise 5 with learned sigma if needed. Implement the actor-critic algorithm below. Perform TD(0) updates at the end of each episode. You can check the training performance plot in the result folder after running the plot cell. Take Figure 2 as a reference training plot. \n",
    "    \n",
    "**Hint:** Check out the PyTorch tutorial from [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) to see how to calculate the $A_\\theta \\Delta_\\theta \\log \\pi_\\theta(a_i|s_i)$ term using the ```detach()``` function. \n",
    "\n",
    "<figure style=\"text-align: center\">\n",
    "<img src=\"imgs/pg_ac.png\" width=\"400px\">\n",
    "<figcaption style=\"text-align: center\"> Figure 2: Training plot of the policy gradient with a critic.\n",
    "</figcaption>\n",
    "</figure>\n",
    "     \n",
    "**Complete the all the unfinished implementation in `PG` class (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Policy Network**: Finish the `__init__(self, state_dim, action_dim)` function and `forward(self, state)` function within the `Policy` class\n",
    "2. **Agent Update Function**: Finish the `update(self, )` function within the `PG` class\n",
    "3. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` function within the `PG` class.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489df71f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4a46183e22c508d922baebfefbe6a0a",
     "grade": true,
     "grade_id": "cell-f54b5f40d174d5b2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4384f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import Video\n",
    "\n",
    "import torch, yaml\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import train as t\n",
    "import utils as u\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6634d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actor-critic agent\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Create a neural network and use it for the mean of the policy.\n",
    "        # The size of the neural network here has been chosen such that \n",
    "        # it is not too big but should perform well in the tasks we want to look at.\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO: Implement actor_logstd as a learnable parameter\n",
    "        # Use log of std to make sure std (standard deviation) of the policy\n",
    "        # doesn't become negative during training\n",
    "        \"\"\"\n",
    "        self.actor_logstd = ...\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimensions of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO: Create a Normal distribution with mean of 'action_mean' & standard deviation of 'action_logstd' \n",
    "        # and return the distribution. You should be using the torch.distributions.Normal class to create the distribution.\n",
    "        \"\"\"\n",
    "        probs = ...\n",
    "\n",
    "        return probs\n",
    "\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.value = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)), nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.value(x).squeeze(1) # output shape [batch,]\n",
    "\n",
    "\n",
    "class PG(object):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        self.name = 'pg'\n",
    "        self.policy = Policy(state_dim, action_dim).to(device)\n",
    "        self.value = Value(state_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()) + list(self.value.parameters()), \n",
    "                                         lr=float(lr),)\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # a simple buffer\n",
    "        self.states = []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "\n",
    "    '''\n",
    "    TODO: Task 1\n",
    "    # Complete the following 4 functions\n",
    "    # Hints: 1. calculate the target values as well as the MSE loss between the predicted value and the target values\n",
    "    #        2. calculate the policy loss (similar to ex5) with advantage calculated from the value function. Normalise\n",
    "    #           the advantage to zero mean and unit variance.\n",
    "    #        3. Use mean to calculate the actor (policy) loss ;)\n",
    "    #           - https://discuss.pytorch.org/t/loss-reduction-sum-vs-mean-when-to-use-each/115641/2\n",
    "    '''\n",
    "    \n",
    "    # 1. Calculate the target values, you should not modify the gradient of the variables\n",
    "    def calculate_target_values(self, rewards, next_states, dones):\n",
    "        ########## Your code starts here. ##########\n",
    "        with torch.no_grad():\n",
    "            next_values = ...\n",
    "            target_values = ...\n",
    "        ########## Your code ends here. ##########\n",
    "        return target_values\n",
    "\n",
    "    # 2. Calculate the critic loss\n",
    "    def calculate_critic_loss(self, values, target_values):\n",
    "        ########## Your code starts here. ##########\n",
    "        critic_loss = ...\n",
    "        ########## Your code ends here. ##########\n",
    "        return critic_loss\n",
    "    \n",
    "    # 3. Advantage estimation, you should not modify the gradient of the variables\n",
    "    def calculate_advantage(self, values, target_values):\n",
    "        ########## Your code starts here. ##########\n",
    "        with torch.no_grad():\n",
    "            advantage = ...\n",
    "        ########## Your code ends here. ##########\n",
    "        return advantage\n",
    "    \n",
    "    # 4. Calculate the actor (policy) loss \n",
    "    def calculate_actor_loss(self, action_probs, advantage):\n",
    "        ########## Your code starts here. ##########\n",
    "        weighted_probs = ...\n",
    "        actor_loss = ...\n",
    "        ########## Your code ends here. ##########\n",
    "        return actor_loss\n",
    "\n",
    "    def update(self,):\n",
    "        action_probs = torch.stack(self.action_probs, dim=0) \\\n",
    "                .to(device).squeeze(-1)\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1)\n",
    "        states = torch.stack(self.states, dim=0).to(device).squeeze(-1)\n",
    "        next_states = torch.stack(self.next_states, dim=0).to(device).squeeze(-1)\n",
    "        dones = torch.stack(self.dones, dim=0).to(device).squeeze(-1)\n",
    "        # clear buffer\n",
    "        self.states, self.action_probs, self.rewards, self.dones, self.next_states = [], [], [], [], []\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Compute the current state value estimates using the value function network\n",
    "        \"\"\"\n",
    "        values = ...\n",
    "        \n",
    "        target_values = self.calculate_target_values(rewards, next_states, dones)        \n",
    "        critic_loss = self.calculate_critic_loss(values, target_values)        \n",
    "        advantage = self.calculate_advantage(values, target_values)\n",
    "        actor_loss = self.calculate_actor_loss(action_probs, advantage)\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: Compute the gradients of the joint loss w.r.t network parameters\n",
    "        \"\"\"\n",
    "        loss = ...\n",
    "        loss.backward()\n",
    "\n",
    "        # Update network parameters using self.optimizer and zero gradients \n",
    "        self.optimizer.step()  \n",
    "        nn.utils.clip_grad_norm_(list(self.policy.parameters())+ list(self.value.parameters()), 1.0)\n",
    "        self.optimizer.zero_grad() \n",
    "\n",
    "        return {}\n",
    "\n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        \"\"\"Return action (np.ndarray) and logprob (torch.Tensor) of this action.\"\"\"\n",
    "        if observation.ndim == 1: observation = observation[None] # add the batch dimension\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "        \n",
    "        '''\n",
    "        # TODO: Task 1\n",
    "        # Hints: 1. the self.policy returns a normal distribution, check the PyTorch documentation to see \n",
    "        #           how to calculate the log_prob of an action and how to sample.\n",
    "        #        2. if evaluating the policy, return policy mean, otherwise, return a sample\n",
    "        #        3. the returned action and the act_logprob should be torch.Tensors.\n",
    "        #           Please always make sure the shape of variables is as you expected.\n",
    "        '''\n",
    "        ########## Your code starts here. ##########\n",
    "        dist = ...\n",
    "        if evaluation:\n",
    "            action = ...\n",
    "        else:\n",
    "            action = ...\n",
    "        \n",
    "        # compute the log probability of the action\n",
    "        act_logprob = ...\n",
    "        ########## Your code ends here. ###########\n",
    "\n",
    "        return action, act_logprob\n",
    "\n",
    "    def record(self, observation, action_prob, next_observation, reward, done):\n",
    "        self.states.append(torch.tensor(observation, dtype=torch.float32))\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
    "        self.dones.append(torch.tensor([done], dtype=torch.float32))\n",
    "        self.next_states.append(torch.tensor(next_observation, dtype=torch.float32))\n",
    "\n",
    "    def load(self, filepath):\n",
    "        d = torch.load(filepath)\n",
    "        self.policy.load_state_dict(d['policy'])\n",
    "        self.value.load_state_dict(d['value'])\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'policy': self.policy.state_dict(),\n",
    "            'value': self.value.state_dict(),\n",
    "        }, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970385d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5af7194a025f5c6a09d1e4bd68084daa",
     "grade": true,
     "grade_id": "cell-9122872aaa786a57",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Policy\n",
    "def test_policy_class():\n",
    "    policy = Policy(4, 1)\n",
    "    x = torch.Tensor((1.,2.,3.,4.)).reshape((1,4))\n",
    "    assert isinstance( policy.forward(x),Normal)\n",
    "    assert policy.actor_logstd.requires_grad\n",
    "\n",
    "test_policy_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267fbc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bd467e6ef4d0bce56458aa52ec2b321",
     "grade": true,
     "grade_id": "cell-43cbda8b39fe293e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "#Test get_action method\n",
    "def test_get_action_1():\n",
    "    cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml'\n",
    "    cfg_args=dict(save_video=False,testing=False,seed=43)\n",
    "    env, cfg = t.setup(cfg_path, cfg_args=cfg_args)\n",
    "    agent = PG(cfg.state_shape[0], cfg.action_dim, cfg.lr, cfg.gamma)\n",
    "    obs = np.array([ 0.00978977, -0.00972236, -0.00706408,  0.00517535])\n",
    "    action = -2.0871510059805587e-05\n",
    "    assert np.allclose(agent.get_action(obs,True)[0].item(),action,atol=1e-03)\n",
    "\n",
    "    agent2 = PG(cfg.state_shape[0], cfg.action_dim, cfg.lr, cfg.gamma)\n",
    "    assert tuple(agent2.get_action(obs,True)[1].shape) == ()\n",
    "\n",
    "test_get_action_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baa5e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70ac43e68631b7ae6e31f11b62e2eece",
     "grade": true,
     "grade_id": "cell-5bfa2cf43da8fc00",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_update_function_1():\n",
    "    cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml'\n",
    "    cfg_args=dict(save_video=False,testing=False,seed=43)\n",
    "    env, cfg = t.setup(cfg_path, cfg_args=cfg_args)\n",
    "    agent = PG(4, cfg.action_dim, cfg.lr, cfg.gamma)\n",
    "\n",
    "    rewards = torch.tensor([1., 1., 1., 1.])\n",
    "    action_probs = torch.tensor([-0.9286, -1.1687, -1.2360, -1.7127])\n",
    "    next_states = torch.tensor([[-0.0079,  0.0071, -0.1703,  0.4061],\n",
    "                                [-0.0274,  0.0524, -0.8029,  1.8464],\n",
    "                                [-0.0633,  0.1342, -0.9942,  2.2525],\n",
    "                                [-0.0931,  0.2017, -0.4978,  1.1568]])\n",
    "    dones = torch.tensor([0., 0., 0., 1.])\n",
    "    states = torch.tensor([[-4.6008e-03, -1.0801e-03,  7.1437e-03, -1.4543e-03],\n",
    "                           [-7.8743e-03,  7.0946e-03, -1.7034e-01,  4.0608e-01],\n",
    "                           [-2.7371e-02,  5.2385e-02, -8.0292e-01,  1.8464e+00],\n",
    "                           [-6.3307e-02,  1.3419e-01, -9.9422e-01,  2.2525e+00]])\n",
    "    values = torch.tensor([-0.0013, -0.0137, -0.0187, -0.0289])\n",
    "\n",
    "    target_values = agent.calculate_target_values(rewards, next_states, dones)        \n",
    "    critic_loss = agent.calculate_critic_loss(values, target_values)        \n",
    "    adv = agent.calculate_advantage(values, target_values)\n",
    "    actor_loss = agent.calculate_actor_loss(action_probs, adv)\n",
    "\n",
    "    assert torch.allclose(target_values, torch.tensor([0.9864, 0.9815, 0.9714, 1.0000]), atol = 0.001)\n",
    "    assert torch.allclose(critic_loss, torch.tensor(1.0012), atol = 0.001)\n",
    "    assert torch.allclose(adv, torch.tensor([-0.6645, -0.2767, -0.5391,  1.4803]), atol = 0.001)\n",
    "    assert torch.allclose(actor_loss, torch.tensor(0.2321), atol = 0.001)\n",
    "\n",
    "test_update_function_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b263dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47065bb40c58f45827cb38b377996854",
     "grade": true,
     "grade_id": "cell-dcf55b2136722f26",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c60f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c35464679797db9e10ee0bb6646ff6e0",
     "grade": true,
     "grade_id": "cell-289c24f5539ad5d3",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b02f2-891d-40c1-b01a-9bd370009a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init agent\n",
    "with open(Path().cwd()/'cfg'/'pg_ac.yaml', 'r') as f:\n",
    "    cfg = u.Struct(**yaml.safe_load(f))\n",
    "    \n",
    "agent = PG(cfg.state_shape[0], cfg.action_dim, cfg.lr, cfg.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.train(agent, cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml', cfg_args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8041f0c-73f8-4f23-bbd2-4a976a8b842a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.plot(cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4922b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    t.test(agent, cfg_path=Path().cwd()/'cfg'/'pg_ac.yaml', cfg_args=dict(save_video=True,testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb556b-187c-4662-a1df-60dc87917930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    video = Video(Path().cwd()/'results'/'InvertedPendulum-v4'/'video'/'test'/'ex6-episode-0.mp4',\n",
    "    embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54640b32",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f83948189f99f90b4085c2d303b547ad",
     "grade": true,
     "grade_id": "cell-09564874b32375cf",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d432792",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13b4216c279dd80ba4512330ce718bdd",
     "grade": false,
     "grade_id": "cell-5db97bc9fe1b8e96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> Relationship between actor-critic and REINFORCE with baseline (10 points) </h3> \n",
    "\n",
    "What is the relationship between actor-critic and REINFORCE with baseline?\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7662c12-4b22-48ca-85cc-d389b77dde79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e350ecacc72df859b8ea1279bba8b20",
     "grade": false,
     "grade_id": "cell-af3dccf8c7e41349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<h5><b>Options</b></h5>\n",
    "\n",
    "1. Both actor-critic and REINFORCE use boostrapping to update their parameters. \n",
    "2. Both methods use Monte-Carlo estimates for updating their parameters.\n",
    "3. The baseline in REINFORCE is the state value function for actor-critic.\n",
    "4. Both methods require full-trajectories before they can update their parameters.\n",
    "5. Both methods can update their parameters during each timestep.\n",
    "6. Actor-critic uses bootstrapping to update its parameters while REINFORCE does not.\n",
    "\n",
    "Select the **two** most appropriate answers. Selecting more than 2 answers results in zero points from this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875db20-b21d-4c28-b2ca-c7f2e46fffe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq1_1 = [] # Select the most approapriate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4810fe-a2b7-44cd-93b1-a1b2d082c2ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3beea4b65c1e4d7a95ab8a1ae1600bb9",
     "grade": false,
     "grade_id": "cell-d838cdbf4808c6ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The cells below are used for autograding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b3310-7e16-4408-9d74-6555c1fc7ed3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1de4c11b92c17597e4ac9050802b3eaa",
     "grade": true,
     "grade_id": "cell-0e00f76e0960536d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 1 <= len(sq1_1) <= 2\n",
    "assert set(sq1_1) < set(range(1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950514e5-56e9-488d-a9b6-b231e7d5262e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1702e61d7c49da20be80181b5c9d064e",
     "grade": true,
     "grade_id": "cell-26b1c613a6f06ff9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dfa77-0d04-4384-84df-88f14550d8b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e76afee0618f7073f2ebef8e947b465b",
     "grade": true,
     "grade_id": "cell-9aa5fe70a03ab88f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b88b08e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a57b56668c6aed15c3d1dcaa0c5e0be7",
     "grade": false,
     "grade_id": "cell-05a96ed4b54d0a0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.2</b> Advantage (5 points) </h3> \n",
    "\n",
    "How can the value of advantage be intuitively interpreted? (Your answer should not be longer than 10 sentences).\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bac64",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c97f49161088ab47f866a42e9dd909a4",
     "grade": true,
     "grade_id": "cell-7ea462e5360ad185",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6c52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c98268593ad7780ce49ad86a1e8f987c",
     "grade": false,
     "grade_id": "cell-9a3f2bca9650b8b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.3</b> Bias and Variance Analysis (10 points) </h3> \n",
    "\n",
    "How does the implemented actor-critic method compare to REINFORCE in terms of bias and variance of the policy gradient estimation? Explain your answer. (Your answer should not be longer than 10 sentences).\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d5916",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22175d03d3f4b28b443696bddd162af1",
     "grade": true,
     "grade_id": "cell-74f7530fd7c8fef3",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddf4e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "600033db53328bf2d16d465b3aff68b2",
     "grade": false,
     "grade_id": "cell-57372114b7deef34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.4</b> Controlling bias-variance tradeoff (10 points) </h3> \n",
    "\n",
    "How could the bias-variance tradeoff in actor-critic be controlled? (Your answer should not be longer than 10 sentences).\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03501249",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "719a83ac1efc28a51317240ab20e3168",
     "grade": true,
     "grade_id": "cell-bd37bcc37ef9fff6",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1e91e-5715-45b9-a2c2-4e8f0fd2fff2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b1c367010d08ca7088a8580a920eb7d",
     "grade": false,
     "grade_id": "cell-5f74898cf5b2c817",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "# 3. Submitting <a id='3.'></a>\n",
    "Ensure all tasks and questions (in ```ex6_DDPG.ipynb``` and ```ex6_PG_AC.ipynb```) are answered and the necessary plots are saved in the appropriate locations. The relevant plots and files needed to be submitted for this assignment are:\n",
    "\n",
    "\n",
    "- Training performance plots:\n",
    "  - `pg_ac.png`: Training performance plots in terms of episode and episodic reward\n",
    "<br>\n",
    "\n",
    "  \n",
    "- Model files:\n",
    "  - `InvertedPendulum-v4_params.pt`: Trained model\n",
    "\n",
    "\n",
    "Ensure the model files and plots are saved in correct paths:\n",
    "- ```results/InvertedPendulum-v4/pg_ac.png``` Training result\n",
    "- ```results/InvertedPendulum-v4/model/InvertedPendulum-v4_params.pt``` Training Model\n",
    "\n",
    "\n",
    "<span style=\"color:red\"> **# IMPORTANT: DO NOT FORGET ANOTHER TASK IN ```ex6_DDPG.ipynb```** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9269a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0c6d7-563a-4037-9d4c-ef1422d7b657",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89e8fd20b8a0b3230d25dc0d69962803",
     "grade": false,
     "grade_id": "cell-99c320f0e224a0c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 3.1 Feedback <a id='3.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e7c42-211b-419b-a3c9-daeab45597f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "016e22b96662eff6447f48f25857a230",
     "grade": false,
     "grade_id": "cell-09e6cdb34c71b984",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa3e0a-f247-43f4-92cc-5b6e99c79049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e49e4f-0803-4d11-9ac4-9f77bedfa52f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c96fba064ac1fc2b00810fc1dc378a0",
     "grade": false,
     "grade_id": "cell-fa64dd33a8c904ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc5ed4-1957-4c5c-97fb-de2dc7c4362e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1 = None   # Question 1.1 Relationship between actor-critic and REINFORCE with baseline (10 points)\n",
    "Q2 = None   # Question 1.2 Advantage (5 points)\n",
    "Q3 = None   # Question 1.3 Bias and Variance Analysis (10 points)\n",
    "Q4 = None   # Question 1.4 Controlling bias-variance tradeoff (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6cdd1-ea58-4a06-a81b-1d7cc36f14a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4dab816afcbe55502451543d8a576129",
     "grade": false,
     "grade_id": "cell-69fa22cf9bfa78b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27706e2f-2a35-4a6e-ad07-9af944b96dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T1 = None   # Implementing DDPG (20 points)\n",
    "Q1 = None   # Question 1.1 Relationship between actor-critic and REINFORCE with baseline (10 points)\n",
    "Q2 = None   # Question 1.2 Advantage (5 points)\n",
    "Q3 = None   # Question 1.3 Bias and Variance Analysis (10 points)\n",
    "Q4 = None   # Question 1.4 Controlling bias-variance tradeoff (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc810cc9-ba38-480d-a95e-178f875bc744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0204b1b962a537ae2e40333c7dfdbcb",
     "grade": false,
     "grade_id": "cell-2e02694ca4fcfa78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "Please share any additional feedback, suggestions, or comments you have about the lecture, assignment, or course content. Your input is valuable in helping us improve the learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d30ad-8d44-4fe5-a642-121200f920d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785254f3-48b6-44a6-ace6-0eae136d6396",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c456a1da33caaefda4a4a4c165be11d6",
     "grade": false,
     "grade_id": "cell-5086c1c7c783ad08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# References <a id='4.'></a>\n",
    "Please use the following section to record references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be24169-8a32-4c87-9e15-3f6eee2364a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
